\chapter{Related Work}

Describe very broadly other approaches to embedding documents. What has been
achieved rather than how some methods work.

Current shortcomings:

\begin{enumerate}

  \item minimal resources about teacher-student (de-facto only SBERT)

  \item no resources on multi-modality

  \item few papers dedicated strictly to embedding documents

  \item fill-in my resources about datasets

\end{enumerate}

\begin{itemize}
  \item Efficient transformers

    \begin{itemize}

      \item Attention mechanisms

        \begin{itemize}
          \item Efficient Transformers~\cite{tay2022efficient} -- nice overview of
            efficient transformers

          \item Longformer~\cite{beltagy2020longformer} -- mention it

          \item BigBird~\cite{zaheer2020big} -- probably has some insights both about
            sparse attentions (only cover very briefly) and about document processing

        \end{itemize}

      \item Attention implementation

        \begin{itemize}

          \item Flash Attention 2 -- mention it, just to show there are approaches that
            completely ditch sparse attentions (such as LLama2 Long)
          \item xFormers -- dtto

          \item Llama 2 Long -- bringing long context to the extreme, only uses full attention

        \end{itemize}

      \item Completely changing the architecture

        \begin{itemize}

          \item Siamese Multi-depth Transformer-base Hierarchical (SMITH)
            encoder~\cite{yang2020beyond} -- approach to long inputs using hierarchies
            rather than changing type of attention

          \item Hierarchical Attention Network (HAN)~\cite{yang2016hierarchical} --
            hierarchical approach along text objects (words, sentences, paragraphs) for
            document classification

        \end{itemize}

    \end{itemize}

  \item Training approaches -- should cover contrastive loss, teacher-student
    and other types of training

    \begin{itemize}

      \item Paragraph Vector~\cite{le2014distributed} -- just mentioned it. It is
        already surpassed methodology.

      \item SBERT~\cite{reimers2019sentence} -- did pretty much the same as us
        (took a model and finetuned it so that the produced embeddings are good),
        except for sentences

      \item MPNet~\cite{song2020mpnet} -- best SBERT model and the model we are
        working with

      \item That paper where SBERT is finetuned on low-resource language with
        teacher-student training

      \item OpenAI embedding paper -- use contrastive loss, mention large batches
        and the memory constraints it brings

      \item Specter~\cite{cohan2020specter} -- focused on scientific documents, use
        of contrastive loss based on citations

      \item Transformer based Multilingual document embedding
        model~\cite{li2020transformer} -- transformer version of LASER, embeds
        documents

      \item LASER~\cite{schwenk2017learning} -- the idea that representations in
        different languages should be close to each other, probably not so relevant
        because they are sentences and also my work only deals with English

    \end{itemize}

  \item Unorthodox document embedding approaches

    \begin{itemize}

      \item Self-Supervised Document Similarity Ranking~\cite{ginzburg2021self} --
        really focused on generating semantically meaningful representations of
        documents, but with an extra cost and complexity


      \item Cross-Document Language Modelling~\cite{caciularu2021cdlm} -- uses
        Longformer for cross-document task. Illustrates that Longformer can be
        flexible and useful for document processing.

    \end{itemize}

  \item Evaluation datasets

    \begin{itemize}

      \item Wikipedia Similarities

      \item ...

    \end{itemize}

\end{itemize}
