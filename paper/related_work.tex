\chapter{Related Work}

% Describe
% - what other work was done in the area of document embedding using transformers
% - focus on the plethora of options, instead of focusing on one single paper
% - what work you continue from, what you follow up on

In this chapter we go over the research that we consider relevant to the
embedding of long pieces of text using the transformer architecture. First we
summarize efforts that have gone into making transformer more efficient so that
it can process long inputs. These advancements are crucial to embedding
documents which are oftentimes much longer than 512 tokens. The next section is
dedicated to typical approaches to training embedding models.

\section{Efficient transformers}\label{section:efficient_transformers}

% - How long inputs=documents are handled by transformers?
% - Why they cannot be handled by classical architectures?
% - What can be done for transformer to handle long texts? -- attention
%    mechanism, attention implementation, different architecture altogether

Though Transformer~\citep{vaswani2017attention} has proven to be performant
architecture (TODO: citations) in the world of NLP it has one inherent
disadvantage when it comes to longer sequences of text. The self-attention,
which is the principal part of the transformer architecture, consumes quadratic
amount of memory in the length of input. This significantly limits Transformer's
applicability to variety of tasks that require longer contexts such as document
retrieval or summarization.

Thanks to the popularity of the transformer architecture, there is a large
amount of research that is focused on making transformers more
efficient~\citep{tay2022efficient}. Most of these efforts fall into one of the
following categories:

\begin{enumerate}

    \item Designing a new memory-efficient attention mechanism,

    \item Using a custom attention implementation, or

    \item Designing new transformer architecture altogether.

\end{enumerate}

We go over each category separately, though these approaches are often
combined. In the section dedicated to custom implementation of self-attention
we also mention commonly used implementation strategies that make transformers
more efficient in practice.

\subsection{Efficient self-attention mechanisms}


As self-attention is the most resource-hungry component of the transformer
architecture, it makes sense to focus on it in order to make the transformer
more efficient. The core of the problem is the multiplication of $N\times d$
query matrix and $N\times d$ key matrix, where $N$ is input length and $d$ is a
dimensionality of the self-attention. Efficient attention mechanisms approximate
this multiplication and thereby avoid computing and storing the $N\times N$
resulting matrix.

\subsubsection{Sparse attention}

Sparse attention approximates the full attention by ignoring dot products between
some query and key vectors. Though it may seem like a crude approximation, there
is research that shows that the full attention focuses mainly on few query-key
vector combinations. For instance \cite{kovaleva2019revealing} shown that full
attentions exhibit only few repeated patterns and that by disabling some
attention heads we can increase the model's performance. Both of these findings
suggest that full attention is over-parametrized and its pruning may be
beneficial. \cite{child2019generating} shown that repeating attention patterns
can be found also when processing images and that by approximating full
attention using such sparse patterns we can increase the efficiency of the model
without sacrificing performance.

Sparse attentions typically compose several attention patterns. One of these
patterns is often full attention that is limited only to a certain neighbourhood
of a given token. This corresponds to findings of \cite{clark2019does}, who
found that full attention gives a lot of focus on previous and next tokens.
Another sparse attention pattern is usually dedicated to enable broader exchange
of information between tokens. In Sparse Transformer~\citep{child2019generating}
distant tokens are connected by sever pre-selected tokens uniformly distributed
throughout the input. In Longformer~\citep{beltagy2020longformer} every token
can attend to every $k$-th distant token to increase its field of vision.
BigBird~\citep{zaheer2020big} computes dot products between randomly chosen
combinations of query and key vectors. These serve as a connecting nodes for
other tokens when they exchange information. The last typical sparse attention
pattern is a global attention that is computed only on a few tokens. Though such
attention pattern is costly it is essential for tasks which require a
representation of the whole input~\citep{beltagy2020longformer}. In Longformer
some significant input tokens such as the \texttt{[CLS]} token, attend to all
other tokens and vice-versa. BigBird additionally computes global attention on
few extra tokens that are added to the input.

Sparse attention pattern doesn't have to be fixed, but can also change
throughout the training. \cite{sukhbaatar2019adaptive} train a transformer that
learns optimal attention span. In their experiments most heads learn to attend
only to few neighbouring tokens and thus make the model more efficient.
Reformer~\citep{kitaev2020reformer} computes the full self-attention only between
close key, query tokens, while letting the model decide which two tokens are
``close'' and which are not. To a certain degree this enables the model to learn
optimal attention patterns between tokens.

\subsubsection{Low-rank approximations and kernel tricks}

Besides sparsifying the attention pattern, there are other techniques to make
the self-attention more efficient in both memory and time.
\cite{wang2020linformer} show that the attention matrix $A :=
\softmax(\frac{QK^T}{d})$ is of low-rank and show that it can be approximated
in less dimensions. By projecting the $(N \times d)$-dimensional key and value
matrices into $(k \times d)$ matrices, where $k << N$ they avoid the expensive
$N\times N$ matrix multiplication. The authors show that empirical performance
of their model is on par with standard transformer models such as
RoBERTa~\citep{liu2019roberta} or BERT~\citep{devlin2019bert}.

In another effort, \cite{choromanski2020rethinking} look at the standard
softmax self-attention through the lens of kernels. Using clever feature
engineering, the authors are able to approximate the elements of the above
mentioned attention matrix $A$ as dot products of query and key feature
vectors. Self-attention can be then approximated as multiplication of four
matrices the projected query and key matrices, the normalization matrix
substituting the division by $d$ and the value matrix. This allows to reorder
the matrix multiplications, first multiplying the projected key and the value
matrix and only after multiplying by the projected query matrix. Such
reordering saves on time and space by a factor of $O(N)$ making the
self-attention linear in input length.

\subsection{Implementation enhancements}

Transformer models can be made more efficient by using various implementation
tricks. As modern hardware gets faster and has more memory, implementation
enhancements can render theoretical advancements such as sparse attentions
unnecessary. For example, \cite{xiong2023effective} train a 70B model on
sequences up to 32K tokens with full self-attention. Nevertheless, the
necessary hardware to train such models is still unavailable to many and
therefore there is still need to use theoretical advancements together with
optimized implementation. For instance \cite{jiang2023mistral} trained an
efficient transformer that uses both sparse attention and its optimized
implementation. The resulting model beats competitive models with twice as many
parameters in several benchmarks.

\subsubsection{Optimized self-attention implementation}

Efficient self-attention implementations view the operation as a whole rather
than a series of matrix multiplications. This enables optimizations that would
not be otherwise possible. The result is a single GPU kernel, that accepts the
query, key and value vectors and outputs the result of a standard
full-attention. \cite{rabe2021self} proposed an implementation of full
self-attention in the Jax
library\footnote{\url{https://github.com/google/jax}} for TPUs that uses
logarithmic amount of memory in the length of the input.
\cite{dao2022flashattention} introduced \emph{Flash Attention} that focuses on
optimizing IO reads and writes and achieves non-trivial speedups. Flash
Attention offers custom CUDA kernels for both block-sparse and full
self-attentions. Later, \cite{dao2023flashattention} improved Flash Attention's
parallelization and increased its the efficiency even more. Though using
optimized kernel is more involved than spelling the operations out, libraries
like xFormers\footnote{\url{https://github.com/facebookresearch/xformers}} and
recent versions of
PyTorch\footnote{\url{https://pytorch.org/docs/2.2/generated/torch.nn.functional.scaled_dot_product_attention.html}}
make it much more straightforward.

\subsubsection{Mixed precision, gradient checkpointing and accumulation}

Besides the above recent implementation enhancements, there are tricks that
have been used for quite some time and not just in conjunction with
transformers. We mention them here, mainly for completeness, since they
dramatically lower the required memory of a transformer model and thus allow
training with longer sequences.

\cite{micikevicius2017mixed} introduced mixed precision training which almost
halves the memory requirements of the model as almost all of the activations
and the gradients are computed in half precision. As the authors show, using
additional techniques such as loss scaling, mixed precision does not worsen the
results compared to traditional single precision training. In another effort to
lower the amount of memory required to train a model, \cite{chen2016training}
introduced gradient checkpointing that can trade speed for memory. With
gradient checkpointing activations of some layers are dropped or overwritten in
order to save memory, but then need to be recomputed again during backward
pass. Another popular technique is gradient accumulation, which may effectively
increase batch size while maintaining the same memory footprint. With gradient
accumulation, gradients of batches are not applied immediately. Instead they
are accumulated for $k$ batches and only then applied to weights. This has a
similar effect as multiplying the batch size by $k$, but not equivalent as
operations like Batch Normalization~\citep{ioffe2015batch} or methods like
in-batch negatives behave differently. Nevertheless gradient accumulation is a
good alternative, especially if the desired batch size cannot fit into the GPU
memory.

\subsection{Combination of model architectures}

To circumvent the problem of memory-hungry self-attention layer, some research
efforts explored combining the transformer architecture with another
architectural concept, namely recursive and hierarchical networks. The common
approach of these models is to not modify the self-attention, nor the maximum
length of input the transformer is able to process, but to instead use
transformers to process smaller pieces of text separately and contextualize
them separately. \cite{dai2019transformer} proposes using recursive
architecture of transformer nodes, where each transformer receives the hidden
states of the previous one. Since gradients do not travel between the nodes,
processing longer sequences requires only constant memory. The resulting model
achieves state of the art performance on language modelling tasks with parameter count
comparable with the competition. Simpler approach was used by
\cite{yang2020beyond}, who proposed a hierarchical model composed of
transformers. First, transformers individually process segments of text
producing segment-level representations, which are together with their position
embeddings fed to another document-level transformer. The authors pretrain with
both word-masking and segment masking losses and together with finetuning on
the target tasks the model beats scores previously set by recurrent networks.


\section{Training document embedding models}

% as we are designing new finetuning/training method to improve embeddings we
% should dedicate a section to how others do it, what other options are there

% highlight how other approaches differ and why they are worse than our method



When we train a document embedding model we aim to improve the performance of
the model's embeddings on downstream tasks. There are many types of downstream
tasks such as classification, retrieval, clustering or visualizations and an
embedding model is generally expected to perform well an all of them. Therefore
there is no clear optimization objective, neither there is an objective which
is universally agreed to outperform others. This makes the field of training
document embedding models particularly interesting and diverse. All training
techniques however, have to adapt to currently available training document
corpora. The selection of supervised corpora of documents is significantly
smaller compared to corpora of shorter sequences such as sentences. This is due
to higher annotation costs and complexity. Nevertheless we may find corpora
of documents linked via citations or links that offer some additional data for
the researches to take advantage of.

 % Compared to shorter text sequences, documents
% contain more inter-connected information making training of their embeddings
% much harder. Also, there are less supervised corpora of documents, as annotation
% of documents is costly and in some cases just too hard for an annotator to do.
% Consider for example specifying how similar are two randomly chosen Wikipedia
% articles on a scale from 0 to 1. In face of these challenges, document embedding
% models are oftentimes trained on unsupervised data, or use its structure as a
% proxy for the documents' properties.

In the simplest case, embedding models are trained only through some kind of
word or token prediction. Paragraph Vector~\citep{le2014distributed} is trained
on prediction of masked-out word given the embeddings of the surrounding words
and of the whole document. However not all models can learn document embedding
through word prediction such as Paragraph Vector. An example of such model is
Transformer~\cite{vaswani2017attention}. Embedding models based on
Transformer's architecture use a combination of pre-training on large
unlabelled corpora, where the model gains most of its knowledge, and a
finetuning method that increases the quality of the model's embeddings.
\cite{cohan2020specter} warm start their embedding model focused on scientific
literature from SciBERT~\citep{beltagy2019scibert}, which was trained using
Masked Language Modelling (or \emph{MLM}) on scientific text.
\cite{neelakantan2022text} use large generative model as an initial checkpoint
for their text embedding model. \cite{izacard2021unsupervised} warm-start their
embedding model from BERT~\citep{devlin2019bert}, which is another model
trained using MLM. These models however differ in how they then finetune their
document embeddings. \cite{cohan2020specter} use a triplet loss that takes
three documents: a query, positive and a negative document. Triplet loss then
minimizes the distance between the query and the positive document while
maximizing the distance between the query and the negative document. To obtain
a positive and a negative document for given query document the authors
leverage the structure of Semantic Scholar
corpus~\citep{ammar2018construction}. \cite{ostendorff2022neighborhood} repeats
the experiment, but shows that with more elaborate method of sampling of
negative papers it is possible to improve the model.

Another popular technique is to train the model by contrasting several inputs'
embedding against each other. Again for each input there is at least one
similar to it (positive), while the others are usually deemed as dissimilar
(negative). The loss would then minimize cross-entropy between the true
similarities and the similarities computed from the input's embeddings. As is
the case with the triplet loss, the main difference between models is how they
obtain the positive and negative documents. \cite{neelakantan2022text} use the
input itself as a positive, while all other inputs in given batch are
considered as negatives. Using in-batch negatives is very efficient since the
model can utilize each input once as a positive and several times as a
negative. As the authors point out, the key of this technique is to have large
batch sizes -- the authors suggest batch sizes of several thousand documents.
\cite{izacard2021unsupervised} obtain positives by augmenting the original
document. The authors also use negatives computed in previous batches. While
using these out-of-batch negatives avoids the need for a large batch size, they
introduce a new set of problems such as making sure that the trained model does
not change too quickly, which would make the stored negatives irrelevant and
possibly harmful to the training. The authors solve this issue by using a
secondary network, that is updated using the parameters of the primary
embedding model.

Though an embedding is usually expected to perform well on any type of task,
\cite{singh2022scirepeval} train an embedding model whose embeddings are
finetuned for a given type of downstream task. The model's task-specific
modules are trained in an end-to-end manner on supervised data collected by the
authors. Thanks to the use of control codes and a layer connecting all
task-specific modules, the proposed model is able to share knowledge across all
types of tasks. While the idea seems reasonable, the authors achieve only small
improvement over state-of-the-art models that generate only single embedding.

\subsection{Comparison to the proposed training method}

As other transformer-based embedding models, ours is also warm-started from a
pre-trained model. However, the following finetuning of the produced embeddings
avoids some of the downsides of the previously mentioned methods. First it does
not require any structure in the training data. This is important as there are
only limited amount of structured corpora of documents. Typically these would
be scientific papers related via citations or Wikipedia articles connected via
links. Our training method allows to use any document corpora such as a set of
books or news articles. Secondly, it does not require large batch sizes.
Despite the advancements mentioned in
Section~\ref{section:efficient_transformers}, using consumer-grade GPU card for
embedding long inputs with transformers can still pose a practical challenge.
In this context, using a batch size of several thousand is unimaginable. Third,
our method does not require to maintain any secondary network while training
the main model. Though, our method uses embeddings of other models, these can
be generated beforehand and thus not take up resources needed to train the main
embedding model. Fourth, our goal is to obtain a model that is applicable to
any type of continuous text. We do not limit our embedding model only to
specific field such as scientific literature. Finally, our model generates a
single embedding which we evaluate on a diverse set of tasks that includes
classification of individual documents, classification of document pairs and
document retrieval.
