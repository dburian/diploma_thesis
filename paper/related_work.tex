\chapter{Related Work}

% Describe
% - what other work was done in the area of document embedding using transformers
% - focus on the plethora of options, instead of focusing on one single paper
% - what work you continue from, what you follow up on

In this chapter we go over the research that we consider relevant to the
embedding of long pieces of text using the transformer architecture. First we
summarize efforts that have gone into making transformer more efficient so that
it can process long inputs. These advancements are crucial to embedding
documents which are oftentimes much longer than 512 tokens. The next section is
dedicated to typical approaches to training embedding models. For completeness,
we also mention uncommon approaches to embedding documents.

\section{Efficient transformers}

% - How long inputs=documents are handled by transformers?
% - Why they cannot be handled by classical architectures?
% - What can be done for transformer to handle long texts? -- attention
%    mechanism, attention implementation, different architecture altogether

Though the Transformer~\citep{vaswani2017attention} has proven to be performant
architecture (TODO: citations) in the world of NLP it has one inherent
disadvantage when it comes to longer sequences of text. The self-attention,
which is the principal part of the transformer architecture, consumes quadratic
amount of memory in the length of input. This significantly limits Transformer's
applicability to variety of tasks that require longer contexts such as document
retrieval or summarization.

Thanks to the popularity of the transformer architecture, there is a large
amount of research that is focused on making transformers more
efficient~\citep{tay2022efficient}. Most of these efforts fall into one of the
following categories:

\begin{enumerate}

    \item Designing a new memory-efficient attention mechanism,

    \item Using a custom attention implementation, or

    \item Designing new transformer architecture altogether.

\end{enumerate}

We go over each category separately, though these approaches are often
combined. In the section dedicated to custom implementation of self-attention
we also mention commonly used implementation strategies that make transformers
more efficient in practice.

\subsection{Efficient self-attention mechanisms}

% Goal of this chapter is to give the reader an idea how long inputs can be
% tackled with transformers and what are the new limitations.
% There is no need to describe each paper/idea in detail, but rather to point
% out to it and say roughly what the paper's importance for my thesis.

% \begin{itemize}

%     \item Efficient Transformers~\cite{tay2022efficient} -- nice overview of
%     efficient transformers

%     \item Longformer~\cite{beltagy2020longformer} -- the easiest implementation
%         of sparse attention

%     \item BigBird~\cite{zaheer2020big} -- has some insights both about
%     sparse attentions (only cover very briefly) and about document processing, a
%         bit more complex approach to attention

%     \item BigBird-ETC

%     \item Sparse transformer~\cite{child2019generating} -- even more complex
%         attention implementation

%     \item Reformer/Performer -- some other transformer that uses other method
%         than just sparsifying attention pattern


% \end{itemize}

As self-attention is the most resource-hungry component of the transformer
architecture, it makes sense to focus on it in order to make the transformer
more efficient. The core of the problem is the multiplication of $N\times d$
query matrix and $N\times d$ key matrix, where $N$ is input length and $d$ is a
dimensionality of the self-attention. Efficient attention mechanisms approximate
this multiplication and thereby avoid computing and storing the $N\times N$
resulting matrix.

\subsubsection{Sparse attention}

% summarize popular findings and models that use sparse attention
%  - what is sparse attention
%  - how models use it
%     - sparse transformer -- propagation of information
%     - longformer -- combination of global token plus local attention
%     - bigbird -- blocks
%  - summarization -- combination of local and global attention

Sparse attention approximates the full attention by ignoring dot products between
some query and key vectors. Though it may seem like a crude approximation, there
is research that shows that the full attention focuses mainly on few query-key
vector combinations. For instance \cite{kovaleva2019revealing} shown that full
attentions exhibit only few repeated patterns and that by disabling some
attention heads we can increase the model's performance. Both of these findings
suggest that full attention is over-parametrized and its pruning may be
beneficial. \cite{child2019generating} shown that repeating attention patterns
can be found also when processing images and that by approximating full
attention using such sparse patterns we can increase the efficiency of the model
without sacrificing performance.

Sparse attentions typically compose several attention patterns. One of these
patterns is often full attention that is limited only to a certain neighbourhood
of a given token. This corresponds to findings of \cite{clark2019does}, who
found that full attention gives a lot of focus on previous and next tokens.
Another sparse attention pattern is usually dedicated to enable broader exchange
of information between tokens. In Sparse Transformer~\citep{child2019generating}
distant tokens are connected by sever pre-selected tokens uniformly distributed
throughout the input. In Longformer~\citep{beltagy2020longformer} every token
can attend to every $k$-th distant token to increase its field of vision.
BigBird~\citep{zaheer2020big} computes dot products between randomly chosen
combinations of query and key vectors. These serve as a connecting nodes for
other tokens when they exchange information. The last typical sparse attention
pattern is some kind of global attention that is computed only on a few tokens.
Though such attention pattern is costly it is essential for tasks which require
a representation of the whole input~\citep{beltagy2020longformer}. In Longformer
some significant input tokens such as the \texttt{[CLS]} token, attend to all
other tokens and vice-versa. BigBird additionally computes global attention on
few extra tokens that are added to the input.

Sparse attention pattern doesn't have to be fixed, but can also change
throughout the training. \cite{sukhbaatar2019adaptive} train a transformer that
learns optimal attention span. In their experiments most heads learn to attend
only to few neighbouring tokens and thus make the model more efficient.
Reformer~\citep{kitaev2020reformer} computes the full self-attention only between
close key, query tokens, while letting the model decide which two tokens are
``close'' and which are not. To a certain degree this enables the model to learn
optimal attention patterns between tokens.

\subsubsection{Low-rank approximations and kernel tricks}

Besides sparsifying the attention pattern, there are other techniques to make
the self-attention more efficient in both memory and time.
\cite{wang2020linformer} show that the attention matrix $A :=
\softmax(\frac{QK^T}{d})$ is of low-rank and show that it can be approximated
in less dimensions. By projecting the $(N \times d)$-dimensional key and value
matrices into $(k \times d)$ matrices, where $k << N$ they avoid the expensive
$N\times N$ matrix multiplication. The authors show that empirical performance
of their model is on par with standard transformer models such as
RoBERTa~\citep{liu2019roberta} or BERT~\citep{devlin2019bert}.

In another effort, \cite{choromanski2020rethinking} look at the standard
softmax self-attention through the lens of kernels. Using clever feature
engineering, the authors are able to approximate the elements of the above
mentioned attention matrix $A$ as dot products of query and key feature
vectors. Self-attention can be then approximated as multiplication of four
matrices the projected query and key matrices, the normalization matrix
substituting the division by $d$ and the value matrix. This allows to reorder
the matrix multiplications, first multiplying the projected key and the value
matrix and only after multiplying by the projected query matrix. Such
reordering saves on time and space by a factor of $O(N)$ making the
self-attention linear in input length.

\subsection{Implementation enhancements}

% \begin{itemize}

%     \item Flash Attention 2 -- mention it, just to show there are approaches that
%     completely ditch sparse attentions (such as LLama2 Long)

%     \item xFormers -- dtto

%     \item Llama 2 Long -- bringing long context to the extreme, only uses full
%         attention

%     \item To make the computation more effective different tricks can be used at
%         the implementation level.

%     \item Although these tricks can be applied to other network architectures as
%         well, they are particularly useful here since they can decrease the
%         memory usage by quite a lot.

%     \item fp16

%     \item gradient checkpointing

%     \item reformer and its reversible stuff

% \end{itemize}

Transformer models can be made more efficient by using various implementation
tricks. As modern hardware gets faster and has more memory, implementation
enhancements can render theoretical advancements such as sparse attentions
unnecessary. For example, \cite{xiong2023effective} train a 70B model on
sequences up to 32K tokens with full self-attention. Nevertheless, the
necessary hardware to train such models is still unavailable to many and
therefore there is still need to use theoretical advancements together with
optimized implementation. For instance \cite{jiang2023mistral} trained an
efficient transformer that uses both sparse attention and its optimized
implementation. The resulting model beats competitive models with twice as many
parameters in several benchmarks.

\subsubsection{Optimized self-attention implementation}

Efficient self-attention implementations view the operation as a whole rather
than a series of matrix multiplications. This enables optimizations that would
not be otherwise possible. The result is a single GPU kernel, that accepts the
query, key and value vectors and outputs the result of a standard
full-attention. \cite{rabe2021self} proposed an implementation of full
self-attention in the Jax
library\footnote{\url{https://github.com/google/jax}} for TPUs that uses
logarithmic amount of memory in the length of the input.
\cite{dao2022flashattention} introduced \emph{Flash Attention} that focuses on
optimizing IO reads and writes and achieves non-trivial speedups. Flash
Attention offers custom CUDA kernels for both block-sparse and full
self-attentions. Later, \cite{dao2023flashattention} improved Flash Attention's
parallelization and increased its the efficiency even more. Though using
optimized kernel is more involved than spelling the operations out, libraries
like xFormers\footnote{\url{https://github.com/facebookresearch/xformers}} and
recent versions of
PyTorch\footnote{\url{https://pytorch.org/docs/2.2/generated/torch.nn.functional.scaled_dot_product_attention.html}}
make it much more straightforward.

\subsubsection{Mixed precision, gradient checkpointing and accumulation}

Besides the above recent implementation enhancements, there are tricks that
have been used for quite some time and not just in conjunction with
transformers. We mention them here, mainly for completeness, since they
dramatically lower the required memory of a transformer model and thus allow
training with longer sequences.

\cite{micikevicius2017mixed} introduced mixed precision training which almost
halves the memory requirements of the model as almost all of the activations
and the gradients are computed in half precision. As the authors show, using
additional techniques such as loss scaling, mixed precision does not worsen the
results compared to traditional single precision training. In another effort to
lower the amount of memory required to train a model, \cite{chen2016training}
introduced gradient checkpointing that can trade speed for memory. With
gradient checkpointing activations of some layers are dropped or overwritten in
order to save memory, but then need to be recomputed again during backward
pass. Another popular technique is gradient accumulation, which may effectively
increase batch size while maintaining the same memory footprint. With gradient
accumulation, gradients of batches are not applied immediately. Instead they
are accumulated for $k$ batches and only then applied to weights. This has a
similar effect as multiplying the batch size by $k$, but not equivalent as
operations like Batch Normalization~\citep{ioffe2015batch} or methods like
in-batch negatives behave differently. Nevertheless gradient accumulation is a
good alternative, especially if the desired batch size cannot fit into the GPU
memory.

\subsection{Combination of model architectures}

To circumvent the problem of memory-hungry self-attention layer, some research
efforts explored combining the transformer architecture with another
architectural concept, namely recursive and hierarchical networks. The common
approach of these models is to not modify the self-attention, nor the maximum
length of input the transformer is able to process, but to instead use
transformers to process smaller pieces of text separately and contextualize
them separately. \cite{dai2019transformer} proposes using recursive
architecture of transformer nodes, where each transformer receives the hidden
states of the previous one. Since gradients do not travel between the nodes,
processing longer sequences requires only constant memory. The resulting model
achieves state of the art performance on language modelling tasks with parameter count
comparable with the competition. Simpler approach was used by
\cite{yang2020beyond}, who proposed a hierarchical model composed of
transformers. First, transformers individually process segments of text
producing segment-level representations, which are together with their position
embeddings fed to another document-level transformer. The authors pretrain with
both word-masking and segment masking losses and together with finetuning on
the target tasks the model beats scores previously set by recurrent networks.


\section{Training approaches}

\begin{itemize}

    \item How can be (long) text embeddings trained?

\end{itemize}

\subsection{Autoregressive Language Modelling}

\begin{itemize}

    \item Standard pre-training for many transformers

    \item Paragraph Vector~\citep{le2014distributed} -- just mentioned it. It is
    already surpassed methodology.


\end{itemize}

\subsection{Siamese networks}


\begin{itemize}

      \item SBERT~\citep{reimers2019sentence} -- did pretty much the same as us
        (took a model and finetuned it so that the produced embeddings are good),
        except for sentences

      \item MPNet~\citep{song2020mpnet} -- best SBERT model and the model we are
        working with

\end{itemize}

\subsection{Knowledge distillation}

\begin{itemize}

      \item That paper where SBERT is finetuned on low-resource language with
          teacher-student training~\citep{reimers2020making}

\end{itemize}

\subsection{Contrastive loss}

\begin{itemize}

      \item OpenAI embedding paper -- use contrastive loss, mention large batches
        and the memory constraints it brings

      \item Specter~\citep{cohan2020specter} -- focused on scientific documents, use
        of contrastive loss based on citations

      \item Transformer based Multilingual document embedding
        model~\citep{li2020transformer} -- transformer version of LASER, embeds
        documents

\end{itemize}

\section{Unorthodox document embedding approaches}

\begin{itemize}

    \item Self-Supervised Document Similarity Ranking~\citep{ginzburg2021self} --
    really focused on generating semantically meaningful representations of
    documents, but with an extra cost and complexity


    \item Cross-Document Language Modelling~\citep{caciularu2021cdlm} -- uses
    Longformer for cross-document task. Illustrates that Longformer can be
    flexible and useful for document processing.

\end{itemize}
