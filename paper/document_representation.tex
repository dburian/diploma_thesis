\chapter{Document representation}

In this chapter we define the properties of a document representation we strive
for. The following definitions create a foundation for both comparing models
and choosing evaluation tasks.

Before enumerating individual properties, we formally define, what we mean by
document representation. Here we follow the concept of embedding, so often
encountered in deep learning; representation of an object is understood as
dense a vector of floating point numbers.

\begin{defn}
  Document representation (or embedding) is a mapping from continuous
  piece of text to a dense vector of floating point numbers.
\end{defn}

The properties align with the goal of this work; which is \emph{to create
semantic embeddings of long texts}. Note that the purpose of this chapter is to
elaborate on this goal by explaining what aspects of document representation we
consider important and why. The aim of this chapter is not to give detailed, or
precise definitions of document representation's attributes, which can be
tested.

The properties are split into two sets: \emph{internal} and \emph{external}.
Internal properties inspect how the representation is computed. External
properties examine the representation itself and how it behaves and what
information it contains.

\section{Internal properties}

\subsection{Long documents}

As our goal is to embed long pieces of texts, it is natural to demand that the
model used to generate the embedding is able to process whole documents. Since
the meaning of a longer text can be distributed unevenly throughout it, the
model should see the entire text before generating an embedding. This gives the
model the chance to decide what is important and what can be left out.


%TODO: change how properties are referenced, numbers are useless.
\begin{repre_prop}[long-inputs]\label{repre_prop:long-inputs}

  Document representation should be able to capture content from the entire
  document, not just its part.

\end{repre_prop}

Notice that we avoided specifying an exact threshold of input's length. While
this would make the property more testable, we believe that in practice no
threshold is truly sufficient and therefore it does not make sense to define it
explicitely.

\subsection{Context and text structure}

It has been shown that natural language tends to be ambiguous, especially when considering
shorter contexts TODO: citation. To capture meaning, the model producing the
embedding should contextualize every piece of text found on its input. In other
words it should have the ability to compare representations of various words
and/or sentences and/or paragraphs with each other.

Considering long context is also beneficial for assessing the text structure,
which we see as an important aspect of a longer text.

\begin{repre_prop}[contextualized-inputs]\label{repre_prop:contextualized-inputs}

  Document representation should be able to capture meaning of document's
  individual parts in the context of the whole document.

\end{repre_prop}

\section{External properties}

\subsection{Semantic similarity}

The goal of this work is to produce document representations which reflect the
meaning of the original document. To enforce this goal we require that
embeddings of documents with similar meaning will be close to each other.

\begin{repre_prop}[similarity-as-proximity]\label{repre_prop:similarity-as-proximity}

  Semantically similar documents should be mapped close to each other in the
  document representation vector space.

\end{repre_prop}

TODO: do not go into this too deeply
TODO: inferencialismus
\subsection{Topic}

Part of semantic meaning of a document?

Faktick√Ω obsah a sentiment?

\subsection{Author}



