\chapter{Document representation}

In this chapter we define the properties of a document representation we strive
for. The following definitions create a foundation both for comparing models
and choosing evaluation tasks.

Before enumerating individual properties, we formally define, what we mean by
document representation. Here we follow the concept of embedding, so often
encountered in deep learning; representation of an object is understood as a
vector of floating point numbers.

\begin{defn}
  Document representation (or embedding) is a mapping from continuous
  piece of text to a dense vector of floating point numbers.
\end{defn}

The properties align with the goal of this work; which is \emph{to create
semantic embeddings of long texts}. While this definition is concise, it lacks
the exactness which will be needed, once we we make decisions about our model.

The properties are split into two sets: \emph{internal} and \emph{external}.
Internal properties inspect how the representation is computed. External
properties examine the representation itself and how it behaves and what
information it contains.

Both types of definitions are difficult to test. As is generally known neural
networks are black boxes, whose computations are often not understood
completely. This makes studying both internal and external properties
challenging and sometimes even impossible. Consequently we try to be cautious
with the properties' definitions and keep the testability of given definition
in mind.


\section{Internal properties}



\subsection{Long documents}

As our goal is to embed long pieces of texts, it is natural to demand that the
model used to generate the embedding is able to process whole documents. Since
the meaning of a longer text can be distributed unevenly throughout it, the
model should see the entire text before generating an embedding. This gives the
model the chance to decide what is important and what can be left out.


%TODO: change how properties are referenced, numbers are useless.
\begin{repre_prop}[long-inputs]\label{repre_prop:long-inputs}

  Document representation should be able to capture content from the entire
  document, not just its part.

\end{repre_prop}

Notice that we avoided specifying an exact threshold of input's length. While
this would make the property more testable, we believe that no threshold is
truly sufficient.

\subsection{Context and text structure}

It has been shown that natural language tends to be ambiguous, especially with
shorter contexts TODO: citation. To capture meaning, the model producing the
embedding should contextualize every piece of text found on its input. In other
words it should have the ability to compare representations of various words
and/or sentences and/or paragraphs with each other. This gives us a chance that
the embedding produced by such model will contain the overall meaning of the
document rather than meaning of its individual parts.

Long context is also important for assessing the text structure, which we see as an important
aspect of a document.
