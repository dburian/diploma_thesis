\chapter{Distilling structural and contextual qualities of document
embeddings}\label{chapter:training_method}

In this chapter we describe our method of finetuning document embeddings. Our
method is based on teacher-student training approach, where we distil the
knowledge of two teacher embedding models into a single student model. In
Section~\ref{section:training_method} we explain our training method in detail
and roughly define the used loss function. Then in
Section~\ref{section:teacher_models} we describe the two teacher models we use
in the rest of the thesis. Finally, in Section~\ref{section:student_model} we
describe the architecture and pre-training of the student model.

\section{Training methodology}\label{section:training_method}

% \begin{itemize}
%     \item what is teacher-student training
%     \item our approach to training is to use teacher-student training
%     \item motivation to do so -- reference document representation chapter
%     \item two teachers -- structural and contextual teacher
% \end{itemize}

Our training methodology aims to train an embedding model such that its
embeddings more faithfully represent the input. As we described in
Chapter~\ref{chapter:document_representation} we distinguish two qualities of
faithful representations: structural and contextual. The goal is to instill
both of these two qualities into a single embedding model. To do so, we use
teacher-student training with two teacher text embedding models, one with high
structural capacity, the other with high contextual capacity.

In the following subsections we describe teacher-student training in detail and
give high-level overview of the proposed loss function.

\subsection{Teacher-student training}

In teacher-student training we train a single \emph{student} model based on a
non-trainable \emph{teacher} model. The goal is to make the student model to
imitate the teacher model and thereby digest the teacher's understanding of the
input. Teacher-student training is useful in a number of situations. For
instance it used to overcome some inherit limitation of the teacher model such
as its large size~\citep{sanh2019distilbert}. Also, teacher-student training
can be used to enforce some kind of alignment between models'
outputs~\citep{reimers2020making}. The motivation to align the model's outputs
often comes from an intuition about language, such as that embeddings of
two translations of a given sentence should be the close to each other.

In our setting, we assume two embedding models $\Teacher_S$, $\Teacher_C$ with
high structural and contextual capacities respectively. Teacher-student
training allows us to instill both of these capacities into a third model
{\Student}, while also avoiding some of the architectural limitations of both
reference models. For convenience we call $\Teacher_S$ \emph{structural
teacher}, $\Teacher_C$ \emph{contextual teacher}, and {\Student}
\emph{student}.


\subsection{Abstract loss formulation}

% \begin{itemize}
%     \item there are two losses corresponding to the two reference models
%     \item weighting the two training losses -- structural model will not be applicable
%         for long inputs
% \end{itemize}

Our loss function should align the output of a student model {\Student} with
outputs of two teacher models $\Teacher_S$, and $\Teacher_C$. We use two
similarity functions $\Loss_S$, $\Loss_C$ that compare the student's embedding
$y_\Student$ with structural teacher embedding $y_{\Teacher_S}$ and with
contextual teacher embedding $y_{\Teacher_C}$ respectively. As we are unsure
which balance of $\Loss_S$ and $\Loss_C$ is optimal we introduce weighting
parameter $\lambda$ that balances the effect of the two losses on the final
loss. In the most general form, we can assume $\lambda$ to be dependent on the
input text $x$, since the performance of the teacher models might vary across
different inputs. In particular, we can expect $\lambda$ to be dependent on the
length of the input, since for shorter inputs the context is minimal and
therefore expendable. The form of the loss as we have described it is defined
in Equation~\ref{eq:abstract_loss}. We explore concrete options for $\Loss_S$,
$\Loss_C$ and $\lambda(x)$ in Chapter~\ref{chapter:experiments}.

\begin{equation}\label{eq:abstract_loss}
  \Loss(
    x,
    y_\Student,
    y_{\Teacher_S},
    y_{\Teacher_C},
    \lambda
  ) =
    \lambda(x) \Loss_S(y_\Student, y_{\Teacher_S}) +
            \Loss_C(y_\Student, y_{\Teacher_S})
\end{equation}

\section{Teacher models}\label{section:teacher_models}

% \begin{itemize}
%     \item what models we used as teachers
%     \item why we chose as we did
% \end{itemize}

In this section we present the teacher models we use during our experiments in
Chapter~\ref{chapter:experiments}. We highlight why we choose the particular
models and how we obtain the their embeddings. We chose
Sentence-BERT~\citep{reimers2020making} as the structural teacher model, and
Paragraph Vector~\citep{le2014distributed} (or \emph{PV}) as the context teacher
model.

\subsection{SBERT}

Sentence-BERT~\citep{reimers2019sentence} is a composition of a
BERT-like~\citep{devlin2019bert} encoder with a mean pooling layer above its
final hidden states. We have chosen SBERT as structural teacher for its
Transformer architecture, which as we have discussed in
Chapter~\ref{chapter:document_representation}, has high structural capacity.
Additionally, SBERT is finetuned on NLI datasets to increase its text
understanding and to produce embeddings which are semantically meaningful.

There are several versions of SBERT, that differ in the base Transformer
encoder, from which is SBERT warm-started and then finetuned. We use SBERT
warm-started from MPNet~\citep{song2020mpnet}, that achieves high scores across
many sentence embedding
benchmarks\footnote{\url{https://sbert.net/docs/pretrained_models.html}}, while
being reasonably small. To be exact we use its Hugging Face implementation
named \\ \texttt{sentence-transformers/all-mpnet-base-v2}.

We generate the embeddings of any training dataset directly without any
additional finetuning.

\subsection{Paragraph Vector}

Paragraph Vector~\cite{le2014distributed} (also known as Doc2Vec) is a simple
text-embedding model that views the input as a Bag of Words (or BoW). Paragraph
Vector is composed of two sub-models called Distributed Memory (DM) and
Distributed Bag of Words (DBOW). In practice one or both models can be used,
where the final embedding is the concatenation of all sub-models' outputs. Both
models construct embedding of the whole input with which they predict a
randomly masked out input word. DM additionally uses embeddings of neighbouring
words.

We choose Paragraph Vector as contextual teacher due to its unique architecture
that forces the model to develop a single vector, that summarizes the common
theme of the document. Moreover, Paragraph Vector does not have limited maximum
input length, and so as a contextual teacher it will always provide some signal
to the student regarding the document's context. Also, even though Paragraph
Vector cannot match the performance of a substantially more complex models such
as Transformers, \cite{dai2015document} show that for larger datasets Paragraph
Vector outperforms classical embedding models such as Latent Dirichlet
Allocation~\citep{blei2003latent} or TF-IDF weighted BoW
model~\citep{harris1954distributional}.

We generate Paragraph Vector's embeddings after training the model on
\emph{all} training datasets for which we require a contextual teacher's
embedding. There are many hyperparameters that govern how Paragraph Vector is
trained. Since there are no universally agreed best-performing values for any
of them, we see these as hyperparameters of our teacher-student training
method. We explore the effect of some of these parameters on the model's
performance in Chapter~\ref{chapter:experiments}.

TODO: my own graphic here

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{./img/pv-dm.png}
    \includegraphics[width=0.4\textwidth]{./img/pv-dbow.png}
    \caption{PV-DM and PV-DBOW architectures.\label{fig:pv-dm_pv-dbow}}
\end{figure}

\section{Student model}\label{section:student_model}


To test our finetuning method we used a student model already pretrained using
Masked Language Modelling on a large corpus. Since our training method relies on
the model's basic natural language understanding, we would have to pre-train the
model either way. Additionally by using a checkpoint of a pretrained model we
dramatically save on resources.

When choosing our model, we valued the following properties. Since we are
limited in resources, we needed the student model to be reasonably small and
memory efficient. Also, as we are embedding documents, the model needed to have
larger maximum input length. We also valued the model's performance, ease of use
and simplicity of the model. In the end we have chosen
Longformer~\citep{beltagy2020longformer} as it is reasonably small, memory
efficient, performs above averege in comparison to other similar
models~\citep{tay2020long} and its self-attention mechanism is straightforward.

We emphasize that our training method can be applied theoretically to any
transformer-based model. We, therefore view this decision as a practical one
that allows us to test our method more easily, rather then theoretical one that
is part of our method.

\subsection{Longformer}

Longformer~\citep{beltagy2020longformer}, is a transformer encoder that is able
to process input sequences longer than traditional Transformer using its
efficient implementation of self-attention.

\subsubsection{Self-attention mechanism}

Longformer uses sparse self-attention that does not compute the full $N \times
N$ attention matrix. Instead Longformer's self-attention relies on composition
of several patterns. We use two patterns: sliding window attention and global
attention. With sliding window attention each token attends to
$\frac{1}{2}\omega$ tokens on its either side. The neighbourhood size $\omega$
can vary across transformer layers. This can be helpful when we try to save on
the number of floating point operations per second (or \emph{FLOPS}) while
minimizing the negative impact on performance~\citep{sukhbaatar2019adaptive}.
With global attention only few tokens selected at runtime attend to all other
tokens. Longformer uses two different set of weights for local and global
attentions. For easier reproducibility of our experiments we avoid the custom
CUDA kernel, and use the author's python block implementation.

\subsubsection{Training}

Longformer is warm-started from a RoBERTa~\citep{liu2019roberta} checkpoint with
its learned positional embeddings copied 8 times to support inputs up to 4096
tokens long. Copying RoBERTa's positional embeddings showed to be an effective
way how to increase the maximum context length of a transformer with learned
positional embeddings. Then it was trained further using MLM on long documents
for 65k gradient steps.

The specially compiled training corpus has some overlap with RoBERTa's
pretraining corpus, but is more focused on longer pieces of text. It includes
the following datasets.

\begin{itemize}
  \item Book corpus~\citep{zhu2015aligning}

  \item English Wikipedia

  \item One third of Realnews dataset~\citep{zellers2019defending}

  \item One third of the Stories corpus~\citep{trinh2018simple}

\end{itemize}
