\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

Text embeddings are the center point of Natural Language Processing
(\emph{NLP}) in machine learning. They allow machine learning models to process
and understand pieces of text by representing them using a vector of numbers.
While we are able to train high-quality word and sentence embeddings, we
struggle to create similarly performant document embeddings. Document
embeddings condense an entire document into a single vector. These
low-dimensional representations can be used to train significantly smaller,
purposely designed models that fulfill a particular task, such as
classification or regression. Thus, document embeddings make any further
computation more efficient. For tasks such as semantic search or clustering,
the increased efficiency of the embeddings is so crucial that the tasks'
computation would be infeasible without it.

Training a document embedding model presents several challenges. Scaling up the
maximum context of Transformers, which are the most performant architecture of
sentence embedding models \citep{reimers2019sentence,gao2021simcse}, requires a
quadratic amount of memory in the number of processed tokens. Despite
relatively recent theoretical advancements such as sparse attention
\citep{child2019generating}, training a Transformer document embedding model
still requires considerable computational resources. Furthermore, current
approaches to training document embedding models either use extremely large
batches \citep{neelakantan2022text}, which further increases the amount of
computational resources necessary, or a complex training setup
\citep{izacard2021unsupervised}. Other approaches
\citep{ostendorff2022neighborhood, cohan2020specter} use datasets with an
inherent structure, such as Wikipedia articles connected via links or academic
papers related via citations. However, as there are only a limited amount of
such datasets, these models lack the universality of an embedding model trained
on a mixture of document formats. Finally, due to the complexity and cost of
document annotation, there are few high-quality supervised long document
datasets. Consequently, researchers settle for datasets with shorter documents
or lower-quality automatic annotation, making the evaluation results less
reliable.

In this work, we tackle some challenges described above while coping with the
rest. We propose a training method that consumes a small amount of resources
and is not dependent on any structure within the training data. We use an
efficient Transformer with sparse attention that can represent texts up to 4096
tokens long with a single vector while having a significantly smaller memory
footprint than a Transformer with full attention. While we do not create a new
dataset that would comply with our high standards, we evaluate our document
embedding model on several tasks covering multiple topics, task types, and
document lengths to make our results more reliable.

Our training method is centered around two embedding models, each having a
distinct quality: Sentence-BERT (\emph{SBERT}) \citep{reimers2019sentence} and
Paragraph Vector \citep{le2014distributed}. SBERT is a sentence embedding
Transformer model that captures the text structure well thanks to its dense
architecture. On the other hand, Paragraph Vector is a much smaller document
embedding model that can embed documents of any length. We see SBERT's capacity
to capture structure and Paragraph Vector's ability to capture extended context
as two complementary qualities desirable to a document embedding model. So, we
combine these qualities and distill them into our document embedding model
using teacher-student training. We train our embedding model to mimic SBERT's
and Paragraph Vector's outputs. In this context, we label the embedding model
as \emph{student}, while SBERT and Paragraph Vector as \emph{teachers}. We
explore several loss functions for each teacher that force the student to mimic
them. We show that both teachers' qualities can benefit the student model's
performance. We evaluate the student model on six classification tasks and two
retrieval tasks. The results show that our method performs consistently and
significantly improves the students' performance over its base checkpoint. Our
method is especially effective in scenarios with few finetuning data available,
where the trained student model outperforms both teachers.

In this thesis, we first describe the motivation behind our method in
Chapter~\ref{chapter:document_representation}. We then review all research we
consider relevant to embedding documents with Transformers in
Chapter~\ref{chapter:related_work}. Chapter~\ref{chapter:training_method}
describes our training method in detail and defines most of the terminology
related to our method. In Chapter~\ref{chapter:experiments}, we go through all
hyperparameters of our method, test different variants, and select those
performing the best on validation tasks. Finally, in
Chapter~\ref{chapter:evaluation}, we train a few student models with the
best-performing variants of our method on a large dataset and evaluate them on
all evaluation tasks. For classification tasks, we evaluate the student models
with different numbers of available finetuning documents and demonstrate their
excellent performance when the amount of finetuning data is limited.
