\chapter*{Conclusions}
\addcontentsline{toc}{chapter}{Conclusions}

In pursuit of our goal to train a Transformer document embedding model with low
computational resources and text corpora, we use a teacher-student training
approach. We combine the qualities of two distinct embedding models and distill
their embeddings into a single student model. For the two teachers, we choose
SBERT \citep{reimers2019sentence} for its capacity to model complex text
structure and Paragraph Vector \citep{le2014distributed} for its unlimited
context. We label the teachers as \emph{structural} and \emph{contextual}
respectively. For our student model, we use an efficient Transformer with
sparse attention. Efficient Transformers compromise between having a large
maximum context and the ability to capture complex text structure. We
initialize the student model with Longformer \citep{beltagy2020longformer}.
However, our technique does not rely on any specific Longformer's features and
can be theoretically applied to any other Transformer with sparse attention,
such as BigBird \citep{zaheer2020big}.

We train the student model on a mixture of two losses, each corresponding to
one teacher. We choose the structural loss to enforce exact similarity with the
structural teacher's embeddings. We experiment with simple functions, such as
Mean Squared Error (\emph{MSE}) and cosine distance, as well as composite
functions, such as contrastive or max-margin losses. Unlike the structural
loss, the contextual loss is designed to give the student more freedom and does
not enforce exact similarity. Thanks to this, we avoid conflict between the two
losses, as structural loss is always given priority. For the contextual loss,
we use a variant of Canonical Correlation Analysis (\emph{CCA})
\citep{hotelling1992relations} called SoftCCA \citep{chen2016training}. SoftCCA
enforces a higher correlation between the student's and the contextual
teacher's embeddings projected via two separate feed-forward networks. We
conduct several experiments to find the best combination of the structural
loss, the contextual teacher, the contextual loss, and the weighting of the
structural and contextual losses. We show that while the contextual loss alone
can improve Longformer's performance, the performance gain is not as
significant as with the structural loss. However, with a suitable
configuration, the student can simultaneously benefit from training on both
losses.

We evaluate the student models on six classification and two retrieval tasks.
With the classification tasks, we show results for three different amounts of
finetuning data available for the classifiers to train on. We demonstrate that
in scenarios with less finetuning data, our training method can boost the
student's performance above the level of both teachers and Longformer. Although
we select tasks with an insignificant percentage of long documents, the results
suggest that an embedding model's large context is relatively unimportant.
Consequently, as the number of finetuning documents grows, the denser
architecture of SBERT gives it a significant advantage over the less capable
architecture of the student model. Nonetheless, the student model significantly
improves Longformer's performance with all three finetuning data limits. In
retrieval tasks, the best student model outperforms SBERT, and all tested
student models again surpass both the contextual teacher and Longformer.

To summarize, we design a method to train document embedding models with small
amount of computational resources that significantly improves the model's
performance. The benefit of our method is especially noticeable in scenarios
with few finetuning data, where the trained model quickly becomes proficient
compared to its base performance.


\section*{Future work}

While this work touches on many research areas, the most opportunities to
extend it lie in the distillation training. We use two losses to distill the
knowledge of two teachers: SBERT and Paragraph Vector. While SBERT is designed
to understand the text structure, Paragraph Vector offers an understanding of
the whole text. However, as the results demonstrate, the best-performing model
uses only SBERT's embeddings and disregards the context that Paragraph Vector
offers as unimportant. Such results offer two ways to extend our work. First,
explore the contribution of Paragraph Vector in detail. In particular, examine
what exactly Paragraph Vector's embedding reflects and how it translates to the
student's embedding with our contextual loss. Despite the many trials we have
carried out, we have found it difficult to measure the effect of the SoftCCA
loss and the configuration of projections used within the contextual loss.
Hence, we think the contextual loss and teacher deserve more attention. Second,
it would be fruitful to experiment with the structural loss that gives the best
model all of its performance. While we carry out some analysis of the
max-margin MSE loss in Section~\ref{section:composite_analysis}, it deserves a
more thorough examination. Max-margin MSE loss is particularly interesting
since many other document embedding models use similar loss to learn from
unsupervised text datasets
\citep{cohan2020specter,ostendorff2022neighborhood,neelakantan2022text,izacard2021unsupervised}.
Some authors highlight that the key to good performance with this type of loss
is the number of negatives it considers
\citep{neelakantan2022text,izacard2021unsupervised}. While the mentioned
techniques require complex setups or large amounts of memory, our approach does
not. In particular, increasing the number of negatives is simple and does not
inflict a larger memory footprint.

Finally, even though we have specific reasons for training with two teacher
models, we propose experimenting with more. In this regard, our method is
easily scalable, and more teachers could result in more consistent performance
of the resulting models. Additionally, as we demonstrate throughout this work,
there are no restrictions on the dimensionality of the teacher's embeddings or
on the teacher's architecture. This offers great freedom and space for original
and unique solutions.
