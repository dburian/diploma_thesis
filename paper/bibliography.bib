%
%  An example of a bibliographical database for bibTeX
%
%  Recommended software for maintenance of *.bib files:
%    JabRef, http://jabref.sourceforge.net/
%
%  BEWARE:
%
%    *  If a name contains a capital letter, which must be kept such,
%       use curly brackets ({T}hailand, {HIV}).
%
%  ===========================================================================

% IMDB
@inproceedings{maas11,
    title = "Learning Word Vectors for Sentiment Analysis",
    author = "Maas, Andrew L.  and
      Daly, Raymond E.  and
      Pham, Peter T.  and
      Huang, Dan  and
      Ng, Andrew Y.  and
      Potts, Christopher",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P11-1015",
    pages = "142--150",
}

% Paragraph Vector
@inproceedings{le2014distributed,
  title={Distributed representations of sentences and documents},
  author={Le, Quoc and Mikolov, Tomas},
  booktitle={International conference on machine learning},
  pages={1188--1196},
  year={2014},
  organization={PMLR}
}

% Paragraph Vector study
@article{dai2015document,
  title={Document embedding with paragraph vectors},
  author={Dai, Andrew M and Olah, Christopher and Le, Quoc V},
  journal={arXiv preprint arXiv:1507.07998},
  year={2015}
}

% word_vectors=1
@article{lau2016empirical,
  title={An empirical evaluation of doc2vec with practical insights into document embedding generation},
  author={Lau, Jey Han and Baldwin, Timothy},
  journal={arXiv preprint arXiv:1607.05368},
  year={2016}
}

% LDA
@article{blei2003latent,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of machine Learning research},
  volume={3},
  number={Jan},
  pages={993--1022},
  year={2003}
}

% TFIDF BoW model
@article{harris1954distributional,
  title={Distributional structure},
  author={Harris, Zellig S},
  journal={Word},
  volume={10},
  number={2-3},
  pages={146--162},
  year={1954},
  publisher={Taylor \& Francis}
}

% Word2Vec
@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

% SBERT
@article{reimers2019sentence,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

% SNLI dataset (SBERT training dataset)
@article{bowman2015large,
  title={A large annotated corpus for learning natural language inference},
  author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:1508.05326},
  year={2015}
}

% Multi-genre NLI dataset (SBERT training dataset)
@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}

% SentEval
@article{conneau2018senteval,
  title={Senteval: An evaluation toolkit for universal sentence representations},
  author={Conneau, Alexis and Kiela, Douwe},
  journal={arXiv preprint arXiv:1803.05449},
  year={2018}
}

% CCA
@incollection{hotelling1992relations,
  title={Relations between two sets of variates},
  author={Hotelling, Harold},
  booktitle={Breakthroughs in statistics: methodology and distribution},
  pages={162--190},
  year={1992},
  publisher={Springer}
}

% DeepCCA
@inproceedings{andrew2013deep,
  title={Deep canonical correlation analysis},
  author={Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
  booktitle={International conference on machine learning},
  pages={1247--1255},
  year={2013},
  organization={PMLR}
}

% DeepCCA with large mini-batches
@inproceedings{wang2015unsupervised,
  title={Unsupervised learning of acoustic features via deep canonical correlation analysis},
  author={Wang, Weiran and Arora, Raman and Livescu, Karen and Bilmes, Jeff A},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4590--4594},
  year={2015},
  organization={IEEE}
}

% SoftCCA
@inproceedings{chang2018scalable,
  title={Scalable and effective deep CCA via soft decorrelation},
  author={Chang, Xiaobin and Xiang, Tao and Hospedales, Timothy M},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1488--1497},
  year={2018}
}

% Longformer
@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

% Longformer's dilatation in CNNs
@article{van2016wavenet,
  title={Wavenet: A generative model for raw audio},
  author={Van Den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray and others},
  journal={arXiv preprint arXiv:1609.03499},
  volume={12},
  year={2016}
}

% BigBird
@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17283--17297},
  year={2020}
}

% Long Range Arena
@article{tay2020long,
  title={Long range arena: A benchmark for efficient transformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  journal={arXiv preprint arXiv:2011.04006},
  year={2020}
}

@article{song2020mpnet,
  title={Mpnet: Masked and permuted pre-training for language understanding},
  author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16857--16867},
  year={2020}
}

% SDR
@article{ginzburg2021self,
  title={Self-supervised document similarity ranking via contextualized language models and hierarchical inference},
  author={Ginzburg, Dvir and Malkiel, Itzik and Barkan, Oren and Caciularu, Avi and Koenigstein, Noam},
  journal={arXiv preprint arXiv:2106.01186},
  year={2021}
}

% Specter
@article{cohan2020specter,
  title={Specter: Document-level representation learning using citation-informed transformers},
  author={Cohan, Arman and Feldman, Sergey and Beltagy, Iz and Downey, Doug and Weld, Daniel S},
  journal={arXiv preprint arXiv:2004.07180},
  year={2020}
}

% SciBERT
@article{beltagy2019scibert,
  title={SciBERT: A pretrained language model for scientific text},
  author={Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  journal={arXiv preprint arXiv:1903.10676},
  year={2019}
}

% Semantic Scholar corpus from SPECTER
@article{ammar2018construction,
  title={Construction of the literature graph in semantic scholar},
  author={Ammar, Waleed and Groeneveld, Dirk and Bhagavatula, Chandra and Beltagy, Iz and Crawford, Miles and Downey, Doug and Dunkelberger, Jason and Elgohary, Ahmed and Feldman, Sergey and Ha, Vu and others},
  journal={arXiv preprint arXiv:1805.02262},
  year={2018}
}

% SciRepEval & Specter2
@article{singh2022scirepeval,
  title={SciRepEval: A Multi-Format Benchmark for Scientific Document Representations},
  author={Singh, Amanpreet and D'Arcy, Mike and Cohan, Arman and Downey, Doug and Feldman, Sergey},
  journal={arXiv preprint arXiv:2211.13308},
  year={2022}
}

@article{caciularu2021cdlm,
  title={CDLM: Cross-document language modeling},
  author={Caciularu, Avi and Cohan, Arman and Beltagy, Iz and Peters, Matthew E and Cattan, Arie and Dagan, Ido},
  journal={arXiv preprint arXiv:2101.00406},
  year={2021}
}

% SMITH
@inproceedings{yang2020beyond,
  title={Beyond 512 tokens: Siamese multi-depth transformer-based hierarchical encoder for long-form document matching},
  author={Yang, Liu and Zhang, Mingyang and Li, Cheng and Bendersky, Michael and Najork, Marc},
  booktitle={Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  pages={1725--1734},
  year={2020}
}

% Transformer version of LASER
@article{li2020transformer,
  title={Transformer based Multilingual document Embedding model},
  author={Li, Wei and Mak, Brian},
  journal={arXiv preprint arXiv:2008.08567},
  year={2020}
}

% LASER
@article{schwenk2017learning,
  title={Learning joint multilingual sentence representations with neural machine translation},
  author={Schwenk, Holger and Douze, Matthijs},
  journal={arXiv preprint arXiv:1704.04154},
  year={2017}
}

%HAN
@inproceedings{yang2016hierarchical,
  title={Hierarchical attention networks for document classification},
  author={Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
  booktitle={Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies},
  pages={1480--1489},
  year={2016}
}

% HAN with transformers
@inproceedings{pappagari2019hierarchical,
  title={Hierarchical transformers for long document classification},
  author={Pappagari, Raghavendra and Zelasko, Piotr and Villalba, Jes{\'u}s and Carmiel, Yishay and Dehak, Najim},
  booktitle={2019 IEEE automatic speech recognition and understanding workshop (ASRU)},
  pages={838--844},
  year={2019},
  organization={IEEE}
}


@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{tay2022efficient,
author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
title = {Efficient Transformers: A Survey},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3530811},
doi = {10.1145/3530811},
abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of “X-former” models have been proposed—Reformer, Linformer, Performer, Longformer, to name a few—which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored “X-former” models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {109},
numpages = {28},
keywords = {attention, neural networks, Transformers, deep learning}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% BERT
@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% Roberta
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

% Research on adaptive attention span in sparse transformers
% Learnable span of full attention
@misc{sukhbaatar2019adaptive,
      title={Adaptive Attention Span in Transformers},
      author={Sainbayar Sukhbaatar and Edouard Grave and Piotr Bojanowski and Armand Joulin},
      year={2019},
      eprint={1905.07799},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Book corpus (part of Longformer and Roberta pretraining corpus)
@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

% Realnews dataset
@article{zellers2019defending,
  title={Defending against neural fake news},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

% stories Corpus
@article{trinh2018simple,
  title={A simple method for commonsense reasoning},
  author={Trinh, Trieu H and Le, Quoc V},
  journal={arXiv preprint arXiv:1806.02847},
  year={2018}
}

% Extending sentence embedding models to other languages using knowledge distillation
@article{reimers2020making,
  title={Making monolingual sentence embeddings multilingual using knowledge distillation},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2004.09813},
  year={2020}
}

% DistilBert
@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

% Sparse transformer by Open AI
@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}


% OpenAI embedding model
@article{neelakantan2022text,
  title={Text and code embeddings by contrastive pre-training},
  author={Neelakantan, Arvind and Xu, Tao and Puri, Raul and Radford, Alec and Han, Jesse Michael and Tworek, Jerry and Yuan, Qiming and Tezak, Nikolas and Kim, Jong Wook and Hallacy, Chris and others},
  journal={arXiv preprint arXiv:2201.10005},
  year={2022}
}

% Reformer
@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

% Overparametrization of BERT (or full-self attentions)
@article{kovaleva2019revealing,
  title={Revealing the dark secrets of BERT},
  author={Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
  journal={arXiv preprint arXiv:1908.08593},
  year={2019}
}

% Full attention considers local neighbourhood a lot
@article{clark2019does,
  title={What does bert look at? an analysis of bert's attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal={arXiv preprint arXiv:1906.04341},
  year={2019}
}


@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

% Performers
@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

% Llama 2, Long
@article{xiong2023effective,
  title={Effective long-context scaling of foundation models},
  author={Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and others},
  journal={arXiv preprint arXiv:2309.16039},
  year={2023}
}

% Mistral
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

% Flash Attention
@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

% Flash attention 2
@article{dao2023flashattention,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

% Google on self-attention implementation
@article{rabe2021self,
  title={Self-attention Does Not Need {$O(n^2)$} Memory},
  author={Rabe, Markus N and Staats, Charles},
  journal={arXiv preprint arXiv:2112.05682},
  year={2021}
}


@article{micikevicius2017mixed,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  journal={arXiv preprint arXiv:1710.03740},
  year={2017}
}

% Gradient checkpointing
@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

% Batch normalization
@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={pmlr}
}

% SIMCSE
@article{gao2021simcse,
  title={Simcse: Simple contrastive learning of sentence embeddings},
  author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  journal={arXiv preprint arXiv:2104.08821},
  year={2021}
}

%--- Embedding models ----
%----- VAE-based -----
@article{holmer2018explaining,
  title={Explaining away syntactic structure in semantic document representations},
  author={Holmer, Erik and Marfurt, Andreas},
  journal={arXiv preprint arXiv:1806.01620},
  year={2018}
}

%----- Transformer-based -----
@article{izacard2021unsupervised,
  title={Unsupervised dense information retrieval with contrastive learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
  journal={arXiv preprint arXiv:2112.09118},
  year={2021}
}

@article{ostendorff2022neighborhood,
  title={Neighborhood contrastive learning for scientific document representations with citation embeddings},
  author={Ostendorff, Malte and Rethmeier, Nils and Augenstein, Isabelle and Gipp, Bela and Rehm, Georg},
  journal={arXiv preprint arXiv:2202.06671},
  year={2022}
}


%----- Old-school architectures -----
@inproceedings{zamani2018neural,
  title={From neural re-ranking to neural ranking: Learning a sparse representation for inverted indexing},
  author={Zamani, Hamed and Dehghani, Mostafa and Croft, W Bruce and Learned-Miller, Erik and Kamps, Jaap},
  booktitle={Proceedings of the 27th ACM international conference on information and knowledge management},
  pages={497--506},
  year={2018}
}

%----- Graph networks -----
@article{xu2021contrastive,
  title={Contrastive document representation learning with graph attention networks},
  author={Xu, Peng and Chen, Xinchi and Ma, Xiaofei and Huang, Zhiheng and Xiang, Bing},
  journal={arXiv preprint arXiv:2110.10778},
  year={2021}
}

% ----- Tasks -------------
% Arxiv papers
@ARTICLE{arxiv_papers,
  author={He, Jun and Wang, Liqun and Liu, Liu and Feng, Jiao and Wu, Hao},
  journal={IEEE Access},
  title={Long Document Classification From Local Word Glimpses via Recurrent Attention Learning},
  year={2019},
  volume={7},
  number={},
  pages={40707-40718},
  doi={10.1109/ACCESS.2019.2907992}
  }

% OC, AAN, S2ORC, PAN
@inproceedings{zhou2020multilevel,
  author={Xuhui Zhou, Nikolaos Pappas, Noah A. Smith},
  title={Multilevel Text Alignment with Cross-Document Attention},
  booktitle={EMNLP},
  year={2020}
}

% AAN
@article{radev2013acl,
  title={The ACL anthology network corpus},
  author={Radev, Dragomir R and Muthukrishnan, Pradeep and Qazvinian, Vahed and Abu-Jbara, Amjad},
  journal={Language Resources and Evaluation},
  volume={47},
  pages={919--944},
  year={2013},
  publisher={Springer}
}

% OC
@article{bhagavatula2018content,
  title={Content-based citation recommendation},
  author={Bhagavatula, Chandra and Feldman, Sergey and Power, Russell and Ammar, Waleed},
  journal={arXiv preprint arXiv:1802.08301},
  year={2018}
}

% S2ORC
@article{lo2019s2orc,
  title={S2ORC: The semantic scholar open research corpus},
  author={Lo, Kyle and Wang, Lucy Lu and Neumann, Mark and Kinney, Rodney and Weld, Dan S},
  journal={arXiv preprint arXiv:1911.02782},
  year={2019}
}

% PAN
@inproceedings{potthast2013overview,
  title={Overview of the 5th international competition on plagiarism detection},
  author={Potthast, Martin and Hagen, Matthias and Gollub, Tim and Tippmann, Martin and Kiesel, Johannes and Rosso, Paolo and Stamatatos, Efstathios and Stein, Benno},
  booktitle={CLEF Conference on Multilingual and Multimodal Information Access Evaluation},
  pages={301--331},
  year={2013},
  organization={CELCT}
}

% IMDB
@inproceedings{maas2011learning,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies},
  pages={142--150},
  year={2011}
}

% Wines & Games
% viz. SDR ginzburg 21
