\chapter{Benchmarks}

In this chapter we will describe a set of benchmarks, which will test our model
and enable us to compare it to other models. First we will describe the tasks
--- datasets and corresponding evaluation metrics, then we will talk about the
models. Results of the benchmarks are discussed in
Chapter~\ref{chapter:evaluation}.

\section{Tasks}

Each task aims to test a different aspect of a model. Our aim was to design a
set of tasks, which can capture a model's capability to embed whole documents.
The major obstacle we faced was the lack of labeled datasets with longer pieces
of text (more than 512 tokens).

TODO: how did we solve the issue

TODO: complete list of task types

\subsubsection{Classification}

Classification tasks test model's capability to separate inputs based on a
complex feature. In our settings, classification tasks can tell us what
information the document embedding contains.


\subsection{IMDB Sentiment Analysis}

IMDB sentiment analysis task is a simple binary classification task. The dataset
contains movie reviews from the Internet Movie
Database\footnote{\url{www.imdb.com}} labaled as either positive or negative.
The dataset is commonly referred to as IMDB classification or sentiment
dataset~\cite{maas11}.

The dataset is split evenly to test and train set, each having 25000 reviews.
The dataset also contains 50000 unlabeled reviews. The label distribution in
both sets is uniform, each of of the two labels is represented by 12500 reviews.

As can be seen from the figure Figure~\ref{fig:imdb_word_token_dist} the reviews
are quite short with only 13.56\% being longer than 512 RoBERTa tokens.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{img/imdb_word_token_distributions.png}
  \caption{Word count and token count distribution of 95-percentiles of
  reviews. The tokens are generated using RoBERTa's pretrained tokenizer from
  HuggingFace}\label{fig:imdb_word_token_dist}
\end{figure}

We included this task to see how our model compares in realtively undemanding
settings, while also evaluating its performence on shorter documents.

\section{Models}

In this section we describe a set of benchmark models our model will be compared
to. All of the benchmark models are able to map a continuous piece of text to a
dense vector representation. This was a requirement as the aim of the evaluation
is to compare different text embeddings. Otherwise we aimed to select a variety
of models with different architectures, learning algorithms and learning tasks.

\subsection{Doc2Vec}

Doc2Vec (also known as Paragraph Vector) introduced in \cite{le2014distributed},
combines slightly altered DBOW and DM architectures previously used by Word2Vec
in \cite{mikolov2013efficient}. As seen in Figure~\ref{fig:pv-dm_pv-dbow} Doc2Vec's
versions of DBOW and DM architectures, called PV-DBOW and PV-DM, incorporate
paragraph's identifier. This allows the architectures to store the information
about the input paragrahs, which is then used as the paragraph's embedding. The
final paragrpah embedding produced by Doc2Vec is linear combination of the
representations of both architectures. Note that, a paragrpah can be any piece
of continuous text.

TODO: my own graphic here

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{./img/pv-dm.png}
    \includegraphics[width=0.4\textwidth]{./img/pv-dbow.png}
    \caption{PV-DM and PV-DBOW architectures.\label{fig:pv-dm_pv-dbow}}
\end{figure}

Doc2Vec is trained using language modelling --- both architectures should
predict a word which is probable in the given context. In PV-DM the context is
paragraph id and the neighbouring words, in PV-DBOW the context is only the
pragraph id.

Advantage of Doc2Vec is its small size and therefore quick learning. Also
Doc2Vec is able to process paragraphs of all lengths. The disadvantage is
that the embedding must be learned even during inference.

\subsection{SBERT}

Sentence-BERT (or SBERT for short) introduced in \cite{reimers2019sentence}, is
a composition of a BERT-like model with pooling layer above its final hidden
states. This architecture is common for sequence classification using a
transformer model. SBERT differs from these simpler approaches, by finetuning on
NLI datasets using siamiese networks. The training setup is depicted in
Figure~\ref{fig:sbert_siemese}. After such training the STS scores of SBERT
embeddings significantly increases.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{./img/sbert_pairs_architectures.png}
    \caption{SBERT architecture with siamiese
    networks.\label{fig:sbert_siemese}}
\end{figure}

The disadvantage of SBERT is its inability to process longer pieces of texts.
The workaround is to either truncate the input or to average multiple embeddings
produced by a sliding window over the input. Both approaches limit the model's
ability to see the document as a whole, which could impair the quality of the
produced embeddings.

\subsection{Longformer}

Longformer introduced in \cite{beltagy2020longformer}, is a transformer with
sparse attention matrix. Whereas in traditional transformer we see dense
attention matrix --- every token ``attends'' to every other token, in
Longformer's attention every token ``attends'' only to selected few global
tokens and neighbouring tokens. Example of such sparse attention matrix is
depicted in Figure~\ref{fig:longformer_sparse_att}. This allows Longformer to
process inputs in linear time of the input length. Thus Longformer is able to
process inputs up to 4096 tokens long.

TODO: maybe comparison to normal dense attention

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{./img/longformer_attention.png}
    \caption{Longformer attention matrix.\label{fig:longformer_sparse_att}}
\end{figure}

While sparse attention matrix allows Longformer to process longer texts, it also
limits its computational power. \cite{zaheer2020big} show that with sparse
attention we need more layers to match the power of a dense attention matrix.

To produce input embeddings we average the embeddings of last hidden states.

TODO: training

\subsection{BigBird}

TODO: how is it different from longformer




