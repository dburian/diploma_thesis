[google_doc_topic]: https://docs.google.com/document/d/13Yb34eyklpX6bGzaf3m0jlsFb8rF10KvLXh4DuY4SD0/edit#heading=h.k2zhq4p261n
[sbert]: https://arxiv.org/pdf/1908.10084.pdf
[longformer]: https://arxiv.org/pdf/2004.05150v2.pdf
[reformer]: https://arxiv.org/pdf/2001.04451.pdf
[jian_22]: https://arxiv.org/pdf/2209.09433.pdf

# Diploma thesis

## Links

- [shared info w/ supervisor][google_doc_topic]

## Topic

In short combine [SBERT][sbert] and either [Longformer][longformer] or
[Reformer][reformer]. The goal is to create contextual embedding of the entire
document.

## Finetuning datsets

- unsupervised datset (Wikipedia) with contrastive loss

Worse than high-quality NLI(Natural Language Inference) datasets.


## Evaluation tasks


## Sources

- [SBERT][sbert]
- [Longformer][longformer]
- [Reformer][reformer]

- [Contrastive learning of Sent. embed. using non-linguistic
  modalities][jian_22]

- current state-of-the-art sentence embeddings:

Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning
of sentence embeddings. In Empirical Methods in Natural Language Processing
(EMNLP), 2021.

## Interesting snippets

- [Jian 2022][jian_22]:

> "we show that Transformer models can generalize better by learning a similar
> task (i.e., clustering) with multi-task losses using non-parallel examples
> from different modalities."





