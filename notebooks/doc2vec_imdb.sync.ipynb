{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adcbd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import Any\n",
    "\n",
    "import datasets\n",
    "\n",
    "import transformer_document_embedding as tde\n",
    "\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")  # Report only TF errors by default\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31ac9082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'transformer_document_embedding' from '/home/dburian/docs/transformer_document_embedding/src/transformer_document_embedding/__init__.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:10:29,620 : INFO : {\"jsonrpc\": \"2.0\", \"method\": \"SyncRequest\", \"params\": {\"data\": {\"file_name\": \"/home/dburian/docs/transformer_document_embedding/notebooks/doc2vec_imdb.sync.py\", \"contents\": \"# ---\\n# jupyter:\\n#  jupytext:\\n#   text_representation:\\n#    extension: .py\\n#    format_name: percent\\n#    format_version: '1.3'\\n#    jupytext_version: 1.3.4\\n#  kernelspec:\\n#   display_name: Python 3\\n#   language: python\\n#   name: python3\\n# ---\\n# %%\\nimport logging\\nimport os\\nfrom typing import Any\\n\\nimport transformer_document_embedding as tde\\n\\nos.environ.setdefault(\\\"TF_CPP_MIN_LOG_LEVEL\\\", \\\"2\\\") # Report only TF errors by default\\n\\nimport tensorflow as tf\\n\\nlogging.basicConfig(\\n  format=\\\"%(asctime)s : %(levelname)s : %(message)s\\\", level=logging.INFO\\n)\\n# %%\\nimport importlib\\n\\nimportlib.reload(tde)\\n\\n# %%\\nLOG_DIR = \\\"../results/notebooks/doc2vec_imdb\\\"\\ntask = tde.tasks.IMDBClassification()\\nmodel = tde.models.IMDBDoc2Vec(log_dir=LOG_DIR)\\n# %%\\nfor i, doc in enumerate(task.train):\\n  if i < 3:\\n    print(doc)\\n  if 25000 < i < 25003:\\n    print(doc)\\n  if i > 25003:\\n    break\\n\\n# %%\\nmodel._doc2vec.train(\\n  task.train,\\n  min_doc_id=25000,\\n  max_doc_id=100000 - 1,\\n  epochs=1,\\n)\\n# %%\\ntf_ds = model._to_tf_dataset(task.train, training=True).shuffle(25000)\\n\\nfor batch in tf_ds.take(5):\\n  print(batch)\\n\\n# %%\\nmodel._cls_head.fit(\\n  tf_ds, epochs=10, callbacks=[tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)]\\n)\\n# %%\\ntest_predictions = model.predict(task.test)\\nresults = task.evaluate(test_predictions)\\n# %%\\nprint(results)\\ntde.evaluation.save_results(results, LOG_DIR)\\n# %%\\nmodel.dv[25000]\\n# %%\\n\\n\\ndef add_feature_vec(doc: dict[str, Any]) -> dict[str, Any]:\\n  feature_vector = model.dv[doc[\\\"id\\\"]]\\n  return {\\\"features\\\": feature_vector, \\\"label\\\": doc[\\\"label\\\"]}\\n\\n\\nsoftmax_train = train.map(add_feature_vec, remove_columns=[\\\"text\\\", \\\"id\\\"]).to_tf_dataset(\\n  1\\n)\\nsoftmax_train = softmax_train.unbatch()\\nsoftmax_train = softmax_train.map(lambda doc: (doc[\\\"features\\\"], doc[\\\"label\\\"]))\\nsoftmax_train = softmax_train.shuffle(1000)\\nsoftmax_train = softmax_train.batch(2)\\n# tf_dataset_softmax_train = tf.data.Dataset.from_generator(\\n#   lambda: softmax_train,\\n#   output_types=(tf.float32, tf.int32),\\n#   output_shapes=((1,), (0,)),\\n# )\\nfor x in softmax_train.take(5):\\n  print(x)\\n# %%\\nsoftmax_net = tf.keras.Sequential(\\n  [\\n    tf.keras.layers.Input(400),\\n    tf.keras.layers.Dense(50, activation=tf.nn.relu),\\n    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid),\\n  ]\\n)\\nsoftmax_net.compile(\\n  optimizer=tf.keras.optimizers.Adam(),\\n  loss=tf.losses.BinaryCrossentropy(),\\n  metrics=[tf.metrics.BinaryAccuracy()],\\n)\\n# %%\\nsoftmax_net.fit(softmax_train)\\n# %%\\n\"}}, \"id\": 1}\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(tde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d3f1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 10:53:56,085 : INFO : using concatenative 8400-dimensional layer1\n",
      "2022-12-21 10:53:56,087 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/c,d400,n5,hs,w10,mc5,s0.001,t6>', 'datetime': '2022-12-21T10:53:56.086992', 'gensim': '4.2.0', 'python': '3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]', 'platform': 'Linux-6.0.11-1-MANJARO-x86_64-with-glibc2.36', 'event': 'created'}\n",
      "2022-12-21 10:53:56,087 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d400,n5,hs,mc5,s0.001,t6>', 'datetime': '2022-12-21T10:53:56.087554', 'gensim': '4.2.0', 'python': '3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]', 'platform': 'Linux-6.0.11-1-MANJARO-x86_64-with-glibc2.36', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "LOG_DIR = \"../results/notebooks/doc2vec_imdb\"\n",
    "task = tde.tasks.IMDBClassification()\n",
    "model = tde.models.IMDBDoc2Vec(log_dir=LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5829b9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 10:53:58,497 : WARNING : Found cached dataset imdb (/home/dburian/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "2022-12-21 10:53:58,523 : WARNING : Loading cached processed dataset at /home/dburian/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-299cf9fbc579d252.arrow\n",
      "2022-12-21 10:54:00,796 : WARNING : Found cached dataset imdb (/home/dburian/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "2022-12-21 10:54:00,820 : WARNING : Loading cached processed dataset at /home/dburian/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-ff884c04faccb420.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0, 'id': 25000}\n",
      "{'text': '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.', 'label': 0, 'id': 25001}\n",
      "{'text': \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\", 'label': 0, 'id': 25002}\n",
      "{'text': 'When I say this is my favourite film of all time, that comment is not to be taken lightly. I probably watch far too many films than is healthy for me, and have loved quite a few of them. I first saw \"La Femme Nikita\" nearly ten years ago, and it still manages to be my absolute favourite. Why?<br /><br />This is more than an incredibly stylish and sexy thriller. Luc Besson\\'s great flair for impeccable direction, fashion, and appropriate usage of music makes this a very watchable film. But it is Anne Parillaud\\'s perfect rendering of a complex character who transforms from a heartless killer into a compassionate, vibrant young woman that makes this film beautiful. I can\\'t keep my eyes off of her when she is on screen.<br /><br />I have seen several of Luc Besson\\'s films including \"Subway\", \"The Professional\", and the irritating \"Fifth Element\", and \"Nikita\" is without a doubt, far superior to any of these. Although this film has tragic elements, it is ultimately extremely hopeful. It is the story of a person who is cruel and merciless, who ultimately comes to realize her own humanity and her own personal power. That, to me is extremely inspiring. If there is hope for Nikita, there is hope for all of us.', 'label': -1, 'id': 50001}\n",
      "{'text': 'I saw this movie because I am a huge fan of the TV series of the same name starring Roy Dupuis and Pet Wilson. The movie was really good and I saw how the TV show is based on the movie. A few episodes of the TV series came directly from the movie and their similarity was amazing. To keep things short, any fan of the movie has to watch the series and any fan of the series must see the original Nikita.', 'label': -1, 'id': 50002}\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(task.train):\n",
    "    if i < 3:\n",
    "        print(doc)\n",
    "    if 25000 < i < 25003:\n",
    "        print(doc)\n",
    "    if i > 25003:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d48db6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 10:54:01,661 : INFO : collecting all words and their counts\n",
      "2022-12-21 10:54:01,662 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2022-12-21 10:54:02,480 : INFO : PROGRESS: at example #10000, processed 2317452 words (2835978 words/s), 152756 word types, 0 tags\n",
      "2022-12-21 10:54:03,333 : INFO : PROGRESS: at example #20000, processed 4663219 words (2750146 words/s), 242583 word types, 0 tags\n",
      "2022-12-21 10:54:04,299 : INFO : PROGRESS: at example #30000, processed 7020384 words (2442625 words/s), 315459 word types, 0 tags\n",
      "2022-12-21 10:54:05,204 : INFO : PROGRESS: at example #40000, processed 9356892 words (2583943 words/s), 380568 word types, 0 tags\n",
      "2022-12-21 10:54:06,044 : INFO : PROGRESS: at example #50000, processed 11731695 words (2830278 words/s), 441275 word types, 0 tags\n",
      "2022-12-21 10:54:06,879 : INFO : PROGRESS: at example #60000, processed 14053055 words (2778745 words/s), 497123 word types, 0 tags\n",
      "2022-12-21 10:54:07,869 : INFO : PROGRESS: at example #70000, processed 16381711 words (2355323 words/s), 550212 word types, 0 tags\n",
      "2022-12-21 10:54:08,304 : WARNING : Highest int doctag (99999) larger than count of documents (75000). This means at least 24999 excess, unused slots (39998400 bytes) will be allocated for vectors.\n",
      "2022-12-21 10:54:08,307 : INFO : collected 576610 word types and 100000 unique tags from a corpus of 75000 examples and 17566362 words\n",
      "2022-12-21 10:54:08,307 : INFO : Creating a fresh vocabulary\n",
      "2022-12-21 10:54:08,684 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 100928 unique words (17.50% of original 576610, drops 475682)', 'datetime': '2022-12-21T10:54:08.684100', 'gensim': '4.2.0', 'python': '3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]', 'platform': 'Linux-6.0.11-1-MANJARO-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}\n",
      "2022-12-21 10:54:08,684 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 16900296 word corpus (96.21% of original 17566362, drops 666066)', 'datetime': '2022-12-21T10:54:08.684658', 'gensim': '4.2.0', 'python': '3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]', 'platform': 'Linux-6.0.11-1-MANJARO-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}\n",
      "2022-12-21 10:54:09,128 : INFO : deleting the raw counts dictionary of 576610 items\n",
      "2022-12-21 10:54:09,139 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2022-12-21 10:54:09,140 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 13178820.067050166 word corpus (78.0%% of prior 16900296)', 'datetime': '2022-12-21T10:54:09.140222', 'gensim': '4.2.0', 'python': '3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]', 'platform': 'Linux-6.0.11-1-MANJARO-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}\n",
      "2022-12-21 10:54:09,196 : INFO : constructing a huffman tree from 100929 words\n",
      "2022-12-21 10:54:12,009 : INFO : built huffman tree with maximum node depth 22\n",
      "2022-12-21 10:54:12,713 : INFO : estimated required memory for 100928 words and 400 dimensions: 7194496000 bytes\n",
      "2022-12-21 10:54:12,714 : INFO : resetting layer weights\n",
      "2022-12-21 10:54:13,022 : INFO : collecting all words and their counts\n",
      "2022-12-21 10:54:13,023 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2022-12-21 10:54:13,795 : INFO : PROGRESS: at example #10000, processed 2317452 words (3005080 words/s), 152756 word types, 0 tags\n",
      "2022-12-21 10:54:14,595 : INFO : PROGRESS: at example #20000, processed 4663219 words (2933317 words/s), 242583 word types, 0 tags\n",
      "2022-12-21 10:54:15,400 : INFO : PROGRESS: at example #30000, processed 7020384 words (2930785 words/s), 315459 word types, 0 tags\n",
      "2022-12-21 10:54:16,210 : INFO : PROGRESS: at example #40000, processed 9356892 words (2887937 words/s), 380568 word types, 0 tags\n",
      "2022-12-21 10:54:17,028 : INFO : PROGRESS: at example #50000, processed 11731695 words (2904069 words/s), 441275 word types, 0 tags\n",
      "2022-12-21 10:54:17,828 : INFO : PROGRESS: at example #60000, processed 14053055 words (2904019 words/s), 497123 word types, 0 tags\n",
      "2022-12-21 10:54:18,632 : INFO : PROGRESS: at example #70000, processed 16381711 words (2899728 words/s), 550212 word types, 0 tags\n",
      "2022-12-21 10:54:19,092 : WARNING : Highest int doctag (99999) larger than count of documents (75000). This means at least 24999 excess, unused slots (39998400 bytes) will be allocated for vectors.\n",
      "2022-12-21 10:54:19,095 : INFO : collected 576610 word types and 100000 unique tags from a corpus of 75000 examples and 17566362 words\n",
      "2022-12-21 10:54:19,096 : INFO : Creating a fresh vocabulary\n",
      "2022-12-21 10:54:19,553 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 100928 unique words (17.50% of original 576610, drops 475682)', 'datetime': '2022-12-21T10:54:19.553891', 'gensim': '4.2.0', 'python': '3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]', 'platform': 'Linux-6.0.11-1-MANJARO-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}\n",
      "2022-12-21 10:54:19,554 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 16900296 word corpus (96.21% of original 17566362, drops 666066)', 'datetime': '2022-12-21T10:54:19.554451', 'gensim': '4.2.0', 'python': '3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]', 'platform': 'Linux-6.0.11-1-MANJARO-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}\n",
      "2022-12-21 10:54:20,007 : INFO : deleting the raw counts dictionary of 576610 items\n",
      "2022-12-21 10:54:20,019 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2022-12-21 10:54:20,019 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 13178820.067050166 word corpus (78.0%% of prior 16900296)', 'datetime': '2022-12-21T10:54:20.019849', 'gensim': '4.2.0', 'python': '3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]', 'platform': 'Linux-6.0.11-1-MANJARO-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}\n",
      "2022-12-21 10:54:20,063 : INFO : constructing a huffman tree from 100929 words\n",
      "2022-12-21 10:54:22,857 : INFO : built huffman tree with maximum node depth 22\n",
      "2022-12-21 10:54:23,558 : INFO : estimated required memory for 100928 words and 400 dimensions: 735104000 bytes\n",
      "2022-12-21 10:54:23,559 : INFO : resetting layer weights\n",
      "2022-12-21 10:54:23,873 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 6 workers on 100929 vocabulary and 8400 features, using sg=0 hs=1 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2022-12-21T10:54:23.873332', 'gensim': '4.2.0', 'python': '3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]', 'platform': 'Linux-6.0.11-1-MANJARO-x86_64-with-glibc2.36', 'event': 'train'}\n",
      "2022-12-21 10:54:25,791 : INFO : EPOCH 0 - PROGRESS: at 0.06% examples, 3951 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:27,406 : INFO : EPOCH 0 - PROGRESS: at 0.43% examples, 14786 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:28,919 : INFO : EPOCH 0 - PROGRESS: at 0.76% examples, 19051 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:30,522 : INFO : EPOCH 0 - PROGRESS: at 1.04% examples, 21051 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:32,082 : INFO : EPOCH 0 - PROGRESS: at 1.39% examples, 22415 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:54:33,527 : INFO : EPOCH 0 - PROGRESS: at 1.75% examples, 23679 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:35,076 : INFO : EPOCH 0 - PROGRESS: at 2.09% examples, 24405 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:36,609 : INFO : EPOCH 0 - PROGRESS: at 2.48% examples, 24978 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:37,619 : INFO : EPOCH 0 - PROGRESS: at 2.74% examples, 25858 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:38,880 : INFO : EPOCH 0 - PROGRESS: at 2.92% examples, 25181 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:39,979 : INFO : EPOCH 0 - PROGRESS: at 3.21% examples, 25777 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:41,115 : INFO : EPOCH 0 - PROGRESS: at 3.52% examples, 26230 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:42,160 : INFO : EPOCH 0 - PROGRESS: at 3.69% examples, 25964 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:43,318 : INFO : EPOCH 0 - PROGRESS: at 3.98% examples, 26343 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:44,713 : INFO : EPOCH 0 - PROGRESS: at 4.26% examples, 26374 words/s, in_qsize 11, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 10:54:45,848 : INFO : EPOCH 0 - PROGRESS: at 4.54% examples, 26723 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:47,000 : INFO : EPOCH 0 - PROGRESS: at 4.71% examples, 26370 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:48,297 : INFO : EPOCH 0 - PROGRESS: at 4.93% examples, 26188 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:49,306 : INFO : EPOCH 0 - PROGRESS: at 5.15% examples, 26314 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:50,334 : INFO : EPOCH 0 - PROGRESS: at 5.38% examples, 26414 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:51,548 : INFO : EPOCH 0 - PROGRESS: at 5.61% examples, 26343 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:52,599 : INFO : EPOCH 0 - PROGRESS: at 5.84% examples, 26421 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:53,666 : INFO : EPOCH 0 - PROGRESS: at 6.05% examples, 26472 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:54,740 : INFO : EPOCH 0 - PROGRESS: at 6.30% examples, 26765 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:55,868 : INFO : EPOCH 0 - PROGRESS: at 6.53% examples, 26753 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:57,178 : INFO : EPOCH 0 - PROGRESS: at 6.84% examples, 26827 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:58,453 : INFO : EPOCH 0 - PROGRESS: at 7.07% examples, 26707 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:54:59,581 : INFO : EPOCH 0 - PROGRESS: at 7.35% examples, 26903 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:00,761 : INFO : EPOCH 0 - PROGRESS: at 7.61% examples, 27040 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:55:01,799 : INFO : EPOCH 0 - PROGRESS: at 7.79% examples, 26888 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:03,061 : INFO : EPOCH 0 - PROGRESS: at 8.06% examples, 26973 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:04,164 : INFO : EPOCH 0 - PROGRESS: at 8.31% examples, 27162 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:05,210 : INFO : EPOCH 0 - PROGRESS: at 8.56% examples, 27200 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:06,425 : INFO : EPOCH 0 - PROGRESS: at 8.79% examples, 27114 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:07,716 : INFO : EPOCH 0 - PROGRESS: at 9.09% examples, 27169 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:09,217 : INFO : EPOCH 0 - PROGRESS: at 9.43% examples, 27245 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:10,235 : INFO : EPOCH 0 - PROGRESS: at 9.74% examples, 27452 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:11,292 : INFO : EPOCH 0 - PROGRESS: at 9.94% examples, 27467 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:12,672 : INFO : EPOCH 0 - PROGRESS: at 10.15% examples, 27298 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:13,815 : INFO : EPOCH 0 - PROGRESS: at 10.42% examples, 27420 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:15,394 : INFO : EPOCH 0 - PROGRESS: at 10.70% examples, 27444 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:16,862 : INFO : EPOCH 0 - PROGRESS: at 11.04% examples, 27530 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:18,414 : INFO : EPOCH 0 - PROGRESS: at 11.35% examples, 27567 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:19,427 : INFO : EPOCH 0 - PROGRESS: at 11.65% examples, 27741 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:20,441 : INFO : EPOCH 0 - PROGRESS: at 11.81% examples, 27636 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:21,444 : INFO : EPOCH 0 - PROGRESS: at 11.98% examples, 27534 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:55:22,847 : INFO : EPOCH 0 - PROGRESS: at 12.26% examples, 27509 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:23,848 : INFO : EPOCH 0 - PROGRESS: at 12.44% examples, 27426 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:24,924 : INFO : EPOCH 0 - PROGRESS: at 12.66% examples, 27421 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:26,167 : INFO : EPOCH 0 - PROGRESS: at 12.98% examples, 27475 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:27,172 : INFO : EPOCH 0 - PROGRESS: at 13.16% examples, 27389 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:28,181 : INFO : EPOCH 0 - PROGRESS: at 13.39% examples, 27418 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:29,253 : INFO : EPOCH 0 - PROGRESS: at 13.68% examples, 27537 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:30,282 : INFO : EPOCH 0 - PROGRESS: at 13.92% examples, 27559 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:31,422 : INFO : EPOCH 0 - PROGRESS: at 14.10% examples, 27427 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:32,534 : INFO : EPOCH 0 - PROGRESS: at 14.39% examples, 27527 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:33,992 : INFO : EPOCH 0 - PROGRESS: at 14.70% examples, 27485 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:34,993 : INFO : EPOCH 0 - PROGRESS: at 14.99% examples, 27625 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:36,021 : INFO : EPOCH 0 - PROGRESS: at 15.16% examples, 27536 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:37,047 : INFO : EPOCH 0 - PROGRESS: at 15.38% examples, 27562 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:55:38,091 : INFO : EPOCH 0 - PROGRESS: at 15.67% examples, 27682 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:39,185 : INFO : EPOCH 0 - PROGRESS: at 15.85% examples, 27578 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:55:40,201 : INFO : EPOCH 0 - PROGRESS: at 16.07% examples, 27601 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:41,543 : INFO : EPOCH 0 - PROGRESS: at 16.33% examples, 27601 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:42,613 : INFO : EPOCH 0 - PROGRESS: at 16.56% examples, 27608 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:43,942 : INFO : EPOCH 0 - PROGRESS: at 16.88% examples, 27613 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:44,972 : INFO : EPOCH 0 - PROGRESS: at 17.17% examples, 27721 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:46,138 : INFO : EPOCH 0 - PROGRESS: at 17.41% examples, 27686 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:47,245 : INFO : EPOCH 0 - PROGRESS: at 17.63% examples, 27670 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:48,582 : INFO : EPOCH 0 - PROGRESS: at 17.87% examples, 27665 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:49,665 : INFO : EPOCH 0 - PROGRESS: at 18.12% examples, 27743 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:50,858 : INFO : EPOCH 0 - PROGRESS: at 18.41% examples, 27787 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:55:52,112 : INFO : EPOCH 0 - PROGRESS: at 18.61% examples, 27719 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:53,158 : INFO : EPOCH 0 - PROGRESS: at 18.81% examples, 27724 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:54,468 : INFO : EPOCH 0 - PROGRESS: at 19.08% examples, 27730 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:55,528 : INFO : EPOCH 0 - PROGRESS: at 19.33% examples, 27814 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:56,585 : INFO : EPOCH 0 - PROGRESS: at 19.57% examples, 27819 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:55:57,734 : INFO : EPOCH 0 - PROGRESS: at 19.76% examples, 27714 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:55:59,257 : INFO : EPOCH 0 - PROGRESS: at 20.11% examples, 27739 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:01,074 : INFO : EPOCH 0 - PROGRESS: at 20.43% examples, 27679 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:02,989 : INFO : EPOCH 0 - PROGRESS: at 20.77% examples, 27594 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:04,528 : INFO : EPOCH 0 - PROGRESS: at 21.12% examples, 27618 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:05,535 : INFO : EPOCH 0 - PROGRESS: at 21.36% examples, 27708 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:06,566 : INFO : EPOCH 0 - PROGRESS: at 21.46% examples, 27569 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:07,644 : INFO : EPOCH 0 - PROGRESS: at 21.72% examples, 27637 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:08,863 : INFO : EPOCH 0 - PROGRESS: at 21.98% examples, 27595 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:56:09,983 : INFO : EPOCH 0 - PROGRESS: at 22.18% examples, 27583 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:11,225 : INFO : EPOCH 0 - PROGRESS: at 22.38% examples, 27536 words/s, in_qsize 11, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 10:56:12,523 : INFO : EPOCH 0 - PROGRESS: at 22.66% examples, 27541 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:14,294 : INFO : EPOCH 0 - PROGRESS: at 23.03% examples, 27505 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:15,862 : INFO : EPOCH 0 - PROGRESS: at 23.35% examples, 27512 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:17,520 : INFO : EPOCH 0 - PROGRESS: at 23.71% examples, 27503 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:19,013 : INFO : EPOCH 0 - PROGRESS: at 24.03% examples, 27529 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:20,026 : INFO : EPOCH 0 - PROGRESS: at 24.29% examples, 27608 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:21,292 : INFO : EPOCH 0 - PROGRESS: at 24.53% examples, 27561 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:22,356 : INFO : EPOCH 0 - PROGRESS: at 24.77% examples, 27562 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:23,617 : INFO : EPOCH 0 - PROGRESS: at 25.05% examples, 27576 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:24,727 : INFO : EPOCH 0 - PROGRESS: at 25.32% examples, 27565 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:25,904 : INFO : EPOCH 0 - PROGRESS: at 25.53% examples, 27538 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:27,188 : INFO : EPOCH 0 - PROGRESS: at 25.77% examples, 27547 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:28,514 : INFO : EPOCH 0 - PROGRESS: at 26.03% examples, 27551 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:29,666 : INFO : EPOCH 0 - PROGRESS: at 26.25% examples, 27534 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:56:30,830 : INFO : EPOCH 0 - PROGRESS: at 26.49% examples, 27515 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:32,018 : INFO : EPOCH 0 - PROGRESS: at 26.72% examples, 27545 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:33,063 : INFO : EPOCH 0 - PROGRESS: at 26.95% examples, 27553 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:34,249 : INFO : EPOCH 0 - PROGRESS: at 27.21% examples, 27586 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:35,279 : INFO : EPOCH 0 - PROGRESS: at 27.37% examples, 27538 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:36,524 : INFO : EPOCH 0 - PROGRESS: at 27.64% examples, 27554 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:37,531 : INFO : EPOCH 0 - PROGRESS: at 27.82% examples, 27512 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:38,533 : INFO : EPOCH 0 - PROGRESS: at 28.13% examples, 27583 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:39,664 : INFO : EPOCH 0 - PROGRESS: at 28.29% examples, 27517 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:40,818 : INFO : EPOCH 0 - PROGRESS: at 28.50% examples, 27501 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:42,347 : INFO : EPOCH 0 - PROGRESS: at 28.83% examples, 27515 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:43,947 : INFO : EPOCH 0 - PROGRESS: at 29.16% examples, 27512 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:45,032 : INFO : EPOCH 0 - PROGRESS: at 29.39% examples, 27564 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:46,347 : INFO : EPOCH 0 - PROGRESS: at 29.59% examples, 27469 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:47,694 : INFO : EPOCH 0 - PROGRESS: at 29.89% examples, 27470 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:49,109 : INFO : EPOCH 0 - PROGRESS: at 30.16% examples, 27455 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:50,773 : INFO : EPOCH 0 - PROGRESS: at 30.49% examples, 27445 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:52,317 : INFO : EPOCH 0 - PROGRESS: at 30.83% examples, 27462 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:53,881 : INFO : EPOCH 0 - PROGRESS: at 31.21% examples, 27471 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:56:55,418 : INFO : EPOCH 0 - PROGRESS: at 31.53% examples, 27486 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:56,940 : INFO : EPOCH 0 - PROGRESS: at 31.83% examples, 27502 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:58,425 : INFO : EPOCH 0 - PROGRESS: at 32.14% examples, 27523 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:56:59,928 : INFO : EPOCH 0 - PROGRESS: at 32.50% examples, 27542 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:01,235 : INFO : EPOCH 0 - PROGRESS: at 32.77% examples, 27548 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:02,355 : INFO : EPOCH 0 - PROGRESS: at 32.99% examples, 27540 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:03,688 : INFO : EPOCH 0 - PROGRESS: at 33.19% examples, 27494 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:04,696 : INFO : EPOCH 0 - PROGRESS: at 33.53% examples, 27551 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:06,130 : INFO : EPOCH 0 - PROGRESS: at 33.79% examples, 27537 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:07,627 : INFO : EPOCH 0 - PROGRESS: at 34.17% examples, 27557 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:09,168 : INFO : EPOCH 0 - PROGRESS: at 34.51% examples, 27566 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:10,255 : INFO : EPOCH 0 - PROGRESS: at 34.82% examples, 27606 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:11,480 : INFO : EPOCH 0 - PROGRESS: at 35.05% examples, 27582 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:12,706 : INFO : EPOCH 0 - PROGRESS: at 35.26% examples, 27557 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:13,751 : INFO : EPOCH 0 - PROGRESS: at 35.54% examples, 27608 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:14,808 : INFO : EPOCH 0 - PROGRESS: at 35.74% examples, 27607 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:57:15,878 : INFO : EPOCH 0 - PROGRESS: at 35.95% examples, 27607 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:17,219 : INFO : EPOCH 0 - PROGRESS: at 36.24% examples, 27606 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:18,415 : INFO : EPOCH 0 - PROGRESS: at 36.51% examples, 27632 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:19,517 : INFO : EPOCH 0 - PROGRESS: at 36.71% examples, 27627 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:20,652 : INFO : EPOCH 0 - PROGRESS: at 36.97% examples, 27658 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:21,966 : INFO : EPOCH 0 - PROGRESS: at 37.19% examples, 27622 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:22,974 : INFO : EPOCH 0 - PROGRESS: at 37.48% examples, 27675 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:24,073 : INFO : EPOCH 0 - PROGRESS: at 37.69% examples, 27669 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:25,110 : INFO : EPOCH 0 - PROGRESS: at 37.89% examples, 27636 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:26,677 : INFO : EPOCH 0 - PROGRESS: at 38.22% examples, 27640 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:57:28,249 : INFO : EPOCH 0 - PROGRESS: at 38.58% examples, 27647 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:29,728 : INFO : EPOCH 0 - PROGRESS: at 38.86% examples, 27665 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:30,789 : INFO : EPOCH 0 - PROGRESS: at 39.11% examples, 27702 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:31,880 : INFO : EPOCH 0 - PROGRESS: at 39.33% examples, 27700 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:32,960 : INFO : EPOCH 0 - PROGRESS: at 39.56% examples, 27699 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:34,356 : INFO : EPOCH 0 - PROGRESS: at 39.81% examples, 27687 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:35,382 : INFO : EPOCH 0 - PROGRESS: at 40.12% examples, 27729 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:36,572 : INFO : EPOCH 0 - PROGRESS: at 40.44% examples, 27752 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:37,744 : INFO : EPOCH 0 - PROGRESS: at 40.61% examples, 27697 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:38,932 : INFO : EPOCH 0 - PROGRESS: at 40.93% examples, 27719 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:40,490 : INFO : EPOCH 0 - PROGRESS: at 41.27% examples, 27726 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:41,960 : INFO : EPOCH 0 - PROGRESS: at 41.61% examples, 27743 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:42,962 : INFO : EPOCH 0 - PROGRESS: at 41.87% examples, 27788 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:43,984 : INFO : EPOCH 0 - PROGRESS: at 42.00% examples, 27755 words/s, in_qsize 12, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 10:57:45,029 : INFO : EPOCH 0 - PROGRESS: at 42.21% examples, 27759 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:57:46,055 : INFO : EPOCH 0 - PROGRESS: at 42.50% examples, 27802 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:47,161 : INFO : EPOCH 0 - PROGRESS: at 42.69% examples, 27761 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:57:48,336 : INFO : EPOCH 0 - PROGRESS: at 42.89% examples, 27746 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:49,779 : INFO : EPOCH 0 - PROGRESS: at 43.21% examples, 27766 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:51,277 : INFO : EPOCH 0 - PROGRESS: at 43.53% examples, 27780 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:52,328 : INFO : EPOCH 0 - PROGRESS: at 43.72% examples, 27747 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:53,496 : INFO : EPOCH 0 - PROGRESS: at 43.96% examples, 27734 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:55,031 : INFO : EPOCH 0 - PROGRESS: at 44.31% examples, 27745 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:57:56,573 : INFO : EPOCH 0 - PROGRESS: at 44.66% examples, 27753 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:58,032 : INFO : EPOCH 0 - PROGRESS: at 44.99% examples, 27773 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:57:59,317 : INFO : EPOCH 0 - PROGRESS: at 45.29% examples, 27780 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:00,362 : INFO : EPOCH 0 - PROGRESS: at 45.49% examples, 27783 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:01,531 : INFO : EPOCH 0 - PROGRESS: at 45.69% examples, 27765 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:02,742 : INFO : EPOCH 0 - PROGRESS: at 45.97% examples, 27779 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:04,004 : INFO : EPOCH 0 - PROGRESS: at 46.23% examples, 27788 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:58:05,116 : INFO : EPOCH 0 - PROGRESS: at 46.43% examples, 27783 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:06,424 : INFO : EPOCH 0 - PROGRESS: at 46.67% examples, 27752 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:08,009 : INFO : EPOCH 0 - PROGRESS: at 46.99% examples, 27754 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:58:09,596 : INFO : EPOCH 0 - PROGRESS: at 47.33% examples, 27754 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:11,160 : INFO : EPOCH 0 - PROGRESS: at 47.68% examples, 27759 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:58:12,661 : INFO : EPOCH 0 - PROGRESS: at 48.04% examples, 27773 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:58:14,149 : INFO : EPOCH 0 - PROGRESS: at 48.40% examples, 27787 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:15,720 : INFO : EPOCH 0 - PROGRESS: at 48.73% examples, 27787 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:17,297 : INFO : EPOCH 0 - PROGRESS: at 49.08% examples, 27791 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:18,820 : INFO : EPOCH 0 - PROGRESS: at 49.41% examples, 27802 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:20,383 : INFO : EPOCH 0 - PROGRESS: at 49.76% examples, 27804 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:21,917 : INFO : EPOCH 0 - PROGRESS: at 50.09% examples, 27811 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:23,522 : INFO : EPOCH 0 - PROGRESS: at 50.46% examples, 27809 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:25,150 : INFO : EPOCH 0 - PROGRESS: at 50.79% examples, 27805 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:58:26,309 : INFO : EPOCH 0 - PROGRESS: at 51.08% examples, 27826 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:27,354 : INFO : EPOCH 0 - PROGRESS: at 51.36% examples, 27859 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:28,597 : INFO : EPOCH 0 - PROGRESS: at 51.48% examples, 27779 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:58:30,199 : INFO : EPOCH 0 - PROGRESS: at 51.80% examples, 27779 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:31,831 : INFO : EPOCH 0 - PROGRESS: at 52.12% examples, 27776 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:33,068 : INFO : EPOCH 0 - PROGRESS: at 52.43% examples, 27811 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:34,128 : INFO : EPOCH 0 - PROGRESS: at 52.63% examples, 27813 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:35,366 : INFO : EPOCH 0 - PROGRESS: at 52.86% examples, 27795 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:36,376 : INFO : EPOCH 0 - PROGRESS: at 53.14% examples, 27830 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:37,587 : INFO : EPOCH 0 - PROGRESS: at 53.35% examples, 27815 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:38,615 : INFO : EPOCH 0 - PROGRESS: at 53.56% examples, 27817 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:39,757 : INFO : EPOCH 0 - PROGRESS: at 53.79% examples, 27807 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:40,832 : INFO : EPOCH 0 - PROGRESS: at 54.04% examples, 27808 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:42,106 : INFO : EPOCH 0 - PROGRESS: at 54.33% examples, 27815 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:43,190 : INFO : EPOCH 0 - PROGRESS: at 54.53% examples, 27815 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:44,535 : INFO : EPOCH 0 - PROGRESS: at 54.77% examples, 27807 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:46,176 : INFO : EPOCH 0 - PROGRESS: at 55.10% examples, 27805 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:47,745 : INFO : EPOCH 0 - PROGRESS: at 55.43% examples, 27806 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:49,516 : INFO : EPOCH 0 - PROGRESS: at 55.77% examples, 27787 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:51,031 : INFO : EPOCH 0 - PROGRESS: at 56.12% examples, 27797 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:52,656 : INFO : EPOCH 0 - PROGRESS: at 56.47% examples, 27796 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:58:53,860 : INFO : EPOCH 0 - PROGRESS: at 56.75% examples, 27836 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:55,179 : INFO : EPOCH 0 - PROGRESS: at 57.01% examples, 27837 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:56,179 : INFO : EPOCH 0 - PROGRESS: at 57.23% examples, 27842 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:57,181 : INFO : EPOCH 0 - PROGRESS: at 57.48% examples, 27849 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:58:58,477 : INFO : EPOCH 0 - PROGRESS: at 57.71% examples, 27824 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:58:59,570 : INFO : EPOCH 0 - PROGRESS: at 57.93% examples, 27821 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:01,037 : INFO : EPOCH 0 - PROGRESS: at 58.19% examples, 27807 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:02,588 : INFO : EPOCH 0 - PROGRESS: at 58.50% examples, 27812 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:04,137 : INFO : EPOCH 0 - PROGRESS: at 58.82% examples, 27815 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:05,694 : INFO : EPOCH 0 - PROGRESS: at 59.13% examples, 27819 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:07,209 : INFO : EPOCH 0 - PROGRESS: at 59.43% examples, 27826 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:08,722 : INFO : EPOCH 0 - PROGRESS: at 59.77% examples, 27834 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:10,299 : INFO : EPOCH 0 - PROGRESS: at 60.10% examples, 27836 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:11,806 : INFO : EPOCH 0 - PROGRESS: at 60.44% examples, 27845 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:13,360 : INFO : EPOCH 0 - PROGRESS: at 60.75% examples, 27850 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:14,779 : INFO : EPOCH 0 - PROGRESS: at 61.09% examples, 27866 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:16,312 : INFO : EPOCH 0 - PROGRESS: at 61.45% examples, 27871 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:59:17,855 : INFO : EPOCH 0 - PROGRESS: at 61.79% examples, 27876 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:59:19,402 : INFO : EPOCH 0 - PROGRESS: at 62.12% examples, 27879 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:20,925 : INFO : EPOCH 0 - PROGRESS: at 62.46% examples, 27888 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:22,499 : INFO : EPOCH 0 - PROGRESS: at 62.75% examples, 27889 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:24,065 : INFO : EPOCH 0 - PROGRESS: at 63.09% examples, 27892 words/s, in_qsize 11, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 10:59:25,580 : INFO : EPOCH 0 - PROGRESS: at 63.41% examples, 27901 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:26,582 : INFO : EPOCH 0 - PROGRESS: at 63.71% examples, 27932 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:27,592 : INFO : EPOCH 0 - PROGRESS: at 63.90% examples, 27912 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:29,053 : INFO : EPOCH 0 - PROGRESS: at 64.16% examples, 27901 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:30,104 : INFO : EPOCH 0 - PROGRESS: at 64.46% examples, 27927 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:31,466 : INFO : EPOCH 0 - PROGRESS: at 64.74% examples, 27924 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:32,553 : INFO : EPOCH 0 - PROGRESS: at 64.95% examples, 27922 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:33,570 : INFO : EPOCH 0 - PROGRESS: at 65.17% examples, 27925 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:34,587 : INFO : EPOCH 0 - PROGRESS: at 65.42% examples, 27930 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:59:35,613 : INFO : EPOCH 0 - PROGRESS: at 65.64% examples, 27932 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:36,970 : INFO : EPOCH 0 - PROGRESS: at 65.94% examples, 27930 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:38,111 : INFO : EPOCH 0 - PROGRESS: at 66.22% examples, 27945 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 10:59:39,198 : INFO : EPOCH 0 - PROGRESS: at 66.41% examples, 27942 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:40,247 : INFO : EPOCH 0 - PROGRESS: at 66.63% examples, 27944 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:41,282 : INFO : EPOCH 0 - PROGRESS: at 66.86% examples, 27947 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:42,312 : INFO : EPOCH 0 - PROGRESS: at 67.08% examples, 27949 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:43,379 : INFO : EPOCH 0 - PROGRESS: at 67.31% examples, 27948 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:44,582 : INFO : EPOCH 0 - PROGRESS: at 67.61% examples, 27956 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:45,861 : INFO : EPOCH 0 - PROGRESS: at 67.88% examples, 27961 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:46,873 : INFO : EPOCH 0 - PROGRESS: at 68.14% examples, 27966 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:48,188 : INFO : EPOCH 0 - PROGRESS: at 68.33% examples, 27943 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:49,228 : INFO : EPOCH 0 - PROGRESS: at 68.61% examples, 27967 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:50,288 : INFO : EPOCH 0 - PROGRESS: at 68.78% examples, 27945 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:51,299 : INFO : EPOCH 0 - PROGRESS: at 69.05% examples, 27971 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:52,329 : INFO : EPOCH 0 - PROGRESS: at 69.22% examples, 27951 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:53,391 : INFO : EPOCH 0 - PROGRESS: at 69.46% examples, 27951 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:54,506 : INFO : EPOCH 0 - PROGRESS: at 69.71% examples, 27946 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:55,520 : INFO : EPOCH 0 - PROGRESS: at 69.94% examples, 27951 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:56,935 : INFO : EPOCH 0 - PROGRESS: at 70.27% examples, 27944 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:58,064 : INFO : EPOCH 0 - PROGRESS: at 70.52% examples, 27939 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 10:59:59,118 : INFO : EPOCH 0 - PROGRESS: at 70.80% examples, 27961 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:00,241 : INFO : EPOCH 0 - PROGRESS: at 70.96% examples, 27934 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:01,338 : INFO : EPOCH 0 - PROGRESS: at 71.17% examples, 27930 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:02,354 : INFO : EPOCH 0 - PROGRESS: at 71.44% examples, 27954 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:03,389 : INFO : EPOCH 0 - PROGRESS: at 71.59% examples, 27933 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:04,608 : INFO : EPOCH 0 - PROGRESS: at 71.81% examples, 27921 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:06,274 : INFO : EPOCH 0 - PROGRESS: at 72.11% examples, 27914 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:07,796 : INFO : EPOCH 0 - PROGRESS: at 72.47% examples, 27919 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:09,410 : INFO : EPOCH 0 - PROGRESS: at 72.82% examples, 27916 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:00:11,011 : INFO : EPOCH 0 - PROGRESS: at 73.17% examples, 27915 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:12,480 : INFO : EPOCH 0 - PROGRESS: at 73.48% examples, 27923 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:14,047 : INFO : EPOCH 0 - PROGRESS: at 73.79% examples, 27922 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:15,614 : INFO : EPOCH 0 - PROGRESS: at 74.07% examples, 27923 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:17,183 : INFO : EPOCH 0 - PROGRESS: at 74.43% examples, 27925 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:18,633 : INFO : EPOCH 0 - PROGRESS: at 74.74% examples, 27937 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:20,086 : INFO : EPOCH 0 - PROGRESS: at 75.06% examples, 27947 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:21,103 : INFO : EPOCH 0 - PROGRESS: at 75.30% examples, 27950 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:22,551 : INFO : EPOCH 0 - PROGRESS: at 75.54% examples, 27919 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:23,631 : INFO : EPOCH 0 - PROGRESS: at 75.80% examples, 27938 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:00:24,808 : INFO : EPOCH 0 - PROGRESS: at 76.07% examples, 27947 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:25,852 : INFO : EPOCH 0 - PROGRESS: at 76.27% examples, 27948 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:26,881 : INFO : EPOCH 0 - PROGRESS: at 76.41% examples, 27931 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:27,957 : INFO : EPOCH 0 - PROGRESS: at 76.73% examples, 27950 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:29,004 : INFO : EPOCH 0 - PROGRESS: at 76.95% examples, 27952 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:30,137 : INFO : EPOCH 0 - PROGRESS: at 77.13% examples, 27927 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:31,628 : INFO : EPOCH 0 - PROGRESS: at 77.44% examples, 27936 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:33,147 : INFO : EPOCH 0 - PROGRESS: at 77.79% examples, 27941 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:34,641 : INFO : EPOCH 0 - PROGRESS: at 78.13% examples, 27949 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:36,224 : INFO : EPOCH 0 - PROGRESS: at 78.48% examples, 27951 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:37,731 : INFO : EPOCH 0 - PROGRESS: at 78.84% examples, 27958 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:39,294 : INFO : EPOCH 0 - PROGRESS: at 79.22% examples, 27961 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:40,845 : INFO : EPOCH 0 - PROGRESS: at 79.59% examples, 27966 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:42,370 : INFO : EPOCH 0 - PROGRESS: at 79.94% examples, 27972 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:00:43,904 : INFO : EPOCH 0 - PROGRESS: at 80.28% examples, 27977 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:45,399 : INFO : EPOCH 0 - PROGRESS: at 80.61% examples, 27983 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:46,886 : INFO : EPOCH 0 - PROGRESS: at 80.96% examples, 27991 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:48,398 : INFO : EPOCH 0 - PROGRESS: at 81.30% examples, 27996 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:49,922 : INFO : EPOCH 0 - PROGRESS: at 81.64% examples, 28001 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:00:51,439 : INFO : EPOCH 0 - PROGRESS: at 81.95% examples, 28007 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:00:52,937 : INFO : EPOCH 0 - PROGRESS: at 82.31% examples, 28014 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:54,430 : INFO : EPOCH 0 - PROGRESS: at 82.68% examples, 28021 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:56,004 : INFO : EPOCH 0 - PROGRESS: at 83.02% examples, 28022 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:00:57,643 : INFO : EPOCH 0 - PROGRESS: at 83.37% examples, 28019 words/s, in_qsize 11, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:00:59,148 : INFO : EPOCH 0 - PROGRESS: at 83.71% examples, 28024 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:00,628 : INFO : EPOCH 0 - PROGRESS: at 84.05% examples, 28032 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:02,205 : INFO : EPOCH 0 - PROGRESS: at 84.41% examples, 28033 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:01:03,721 : INFO : EPOCH 0 - PROGRESS: at 84.77% examples, 28039 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:05,269 : INFO : EPOCH 0 - PROGRESS: at 85.10% examples, 28043 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:06,744 : INFO : EPOCH 0 - PROGRESS: at 85.41% examples, 28049 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:08,267 : INFO : EPOCH 0 - PROGRESS: at 85.75% examples, 28055 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:09,798 : INFO : EPOCH 0 - PROGRESS: at 86.06% examples, 28059 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:11,334 : INFO : EPOCH 0 - PROGRESS: at 86.43% examples, 28064 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:12,847 : INFO : EPOCH 0 - PROGRESS: at 86.79% examples, 28070 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:14,455 : INFO : EPOCH 0 - PROGRESS: at 87.11% examples, 28069 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:01:15,950 : INFO : EPOCH 0 - PROGRESS: at 87.39% examples, 28075 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:01:17,019 : INFO : EPOCH 0 - PROGRESS: at 87.68% examples, 28092 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:01:18,197 : INFO : EPOCH 0 - PROGRESS: at 87.96% examples, 28103 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:19,262 : INFO : EPOCH 0 - PROGRESS: at 88.12% examples, 28084 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:20,262 : INFO : EPOCH 0 - PROGRESS: at 88.35% examples, 28089 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:21,374 : INFO : EPOCH 0 - PROGRESS: at 88.66% examples, 28104 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:22,381 : INFO : EPOCH 0 - PROGRESS: at 88.95% examples, 28126 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:23,635 : INFO : EPOCH 0 - PROGRESS: at 89.11% examples, 28095 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:24,986 : INFO : EPOCH 0 - PROGRESS: at 89.37% examples, 28093 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:26,044 : INFO : EPOCH 0 - PROGRESS: at 89.66% examples, 28111 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:27,075 : INFO : EPOCH 0 - PROGRESS: at 89.87% examples, 28113 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:28,198 : INFO : EPOCH 0 - PROGRESS: at 90.11% examples, 28107 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:29,380 : INFO : EPOCH 0 - PROGRESS: at 90.34% examples, 28100 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:30,959 : INFO : EPOCH 0 - PROGRESS: at 90.65% examples, 28098 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:32,510 : INFO : EPOCH 0 - PROGRESS: at 90.99% examples, 28101 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:34,055 : INFO : EPOCH 0 - PROGRESS: at 91.32% examples, 28104 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:01:35,667 : INFO : EPOCH 0 - PROGRESS: at 91.65% examples, 28101 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:01:37,259 : INFO : EPOCH 0 - PROGRESS: at 91.98% examples, 28101 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:38,721 : INFO : EPOCH 0 - PROGRESS: at 92.30% examples, 28109 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:40,351 : INFO : EPOCH 0 - PROGRESS: at 92.66% examples, 28108 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:41,833 : INFO : EPOCH 0 - PROGRESS: at 92.95% examples, 28098 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:43,411 : INFO : EPOCH 0 - PROGRESS: at 93.28% examples, 28099 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:44,543 : INFO : EPOCH 0 - PROGRESS: at 93.57% examples, 28111 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:45,673 : INFO : EPOCH 0 - PROGRESS: at 93.84% examples, 28123 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:47,037 : INFO : EPOCH 0 - PROGRESS: at 94.09% examples, 28120 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:48,313 : INFO : EPOCH 0 - PROGRESS: at 94.37% examples, 28123 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:49,478 : INFO : EPOCH 0 - PROGRESS: at 94.66% examples, 28133 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:50,735 : INFO : EPOCH 0 - PROGRESS: at 94.87% examples, 28121 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:51,838 : INFO : EPOCH 0 - PROGRESS: at 95.17% examples, 28133 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:53,052 : INFO : EPOCH 0 - PROGRESS: at 95.45% examples, 28140 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:54,303 : INFO : EPOCH 0 - PROGRESS: at 95.71% examples, 28146 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:55,484 : INFO : EPOCH 0 - PROGRESS: at 95.91% examples, 28138 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:56,739 : INFO : EPOCH 0 - PROGRESS: at 96.19% examples, 28143 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:57,910 : INFO : EPOCH 0 - PROGRESS: at 96.40% examples, 28135 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:01:58,943 : INFO : EPOCH 0 - PROGRESS: at 96.67% examples, 28152 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:00,068 : INFO : EPOCH 0 - PROGRESS: at 96.89% examples, 28147 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:02:01,287 : INFO : EPOCH 0 - PROGRESS: at 97.16% examples, 28153 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:02,408 : INFO : EPOCH 0 - PROGRESS: at 97.41% examples, 28164 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:03,603 : INFO : EPOCH 0 - PROGRESS: at 97.63% examples, 28155 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:04,711 : INFO : EPOCH 0 - PROGRESS: at 97.88% examples, 28152 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:02:05,902 : INFO : EPOCH 0 - PROGRESS: at 98.19% examples, 28160 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:07,142 : INFO : EPOCH 0 - PROGRESS: at 98.44% examples, 28164 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:08,545 : INFO : EPOCH 0 - PROGRESS: at 98.72% examples, 28159 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:10,100 : INFO : EPOCH 0 - PROGRESS: at 99.09% examples, 28162 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:02:11,608 : INFO : EPOCH 0 - PROGRESS: at 99.40% examples, 28166 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:13,085 : INFO : EPOCH 0 - PROGRESS: at 99.74% examples, 28172 words/s, in_qsize 5, out_qsize 1\n",
      "2022-12-21 11:02:13,630 : INFO : EPOCH 0: training on 17566362 raw words (13254521 effective words) took 469.8s, 28216 effective words/s\n",
      "2022-12-21 11:02:13,694 : INFO : Doc2Vec lifecycle event {'msg': 'training on 17566362 raw words (13254521 effective words) took 469.8s, 28212 effective words/s', 'datetime': '2022-12-21T11:02:13.694212', 'gensim': '4.2.0', 'python': '3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]', 'platform': 'Linux-6.0.11-1-MANJARO-x86_64-with-glibc2.36', 'event': 'train'}\n",
      "2022-12-21 11:02:13,695 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 6 workers on 100929 vocabulary and 400 features, using sg=1 hs=1 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2022-12-21T11:02:13.695756', 'gensim': '4.2.0', 'python': '3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]', 'platform': 'Linux-6.0.11-1-MANJARO-x86_64-with-glibc2.36', 'event': 'train'}\n",
      "2022-12-21 11:02:14,708 : INFO : EPOCH 0 - PROGRESS: at 1.75% examples, 227295 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:02:15,728 : INFO : EPOCH 0 - PROGRESS: at 4.93% examples, 315195 words/s, in_qsize 11, out_qsize 1\n",
      "2022-12-21 11:02:16,744 : INFO : EPOCH 0 - PROGRESS: at 8.11% examples, 349886 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:02:17,755 : INFO : EPOCH 0 - PROGRESS: at 11.18% examples, 365471 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:18,794 : INFO : EPOCH 0 - PROGRESS: at 14.39% examples, 371062 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:19,796 : INFO : EPOCH 0 - PROGRESS: at 17.61% examples, 378467 words/s, in_qsize 11, out_qsize 1\n",
      "2022-12-21 11:02:20,828 : INFO : EPOCH 0 - PROGRESS: at 20.77% examples, 383719 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:21,851 : INFO : EPOCH 0 - PROGRESS: at 23.82% examples, 385295 words/s, in_qsize 12, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:02:22,896 : INFO : EPOCH 0 - PROGRESS: at 26.99% examples, 387843 words/s, in_qsize 11, out_qsize 1\n",
      "2022-12-21 11:02:23,913 : INFO : EPOCH 0 - PROGRESS: at 30.16% examples, 390373 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:24,945 : INFO : EPOCH 0 - PROGRESS: at 33.12% examples, 390077 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:02:26,005 : INFO : EPOCH 0 - PROGRESS: at 36.39% examples, 390675 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:02:27,020 : INFO : EPOCH 0 - PROGRESS: at 39.44% examples, 392026 words/s, in_qsize 11, out_qsize 1\n",
      "2022-12-21 11:02:28,049 : INFO : EPOCH 0 - PROGRESS: at 42.63% examples, 392717 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:29,054 : INFO : EPOCH 0 - PROGRESS: at 45.69% examples, 393547 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:30,054 : INFO : EPOCH 0 - PROGRESS: at 48.79% examples, 394328 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:31,065 : INFO : EPOCH 0 - PROGRESS: at 51.89% examples, 394877 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:32,081 : INFO : EPOCH 0 - PROGRESS: at 54.96% examples, 395917 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:02:33,100 : INFO : EPOCH 0 - PROGRESS: at 58.09% examples, 396450 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:34,131 : INFO : EPOCH 0 - PROGRESS: at 61.09% examples, 396728 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:35,131 : INFO : EPOCH 0 - PROGRESS: at 64.30% examples, 397971 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:36,137 : INFO : EPOCH 0 - PROGRESS: at 67.44% examples, 398593 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:37,165 : INFO : EPOCH 0 - PROGRESS: at 70.52% examples, 397888 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:02:38,174 : INFO : EPOCH 0 - PROGRESS: at 73.56% examples, 398306 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:02:39,190 : INFO : EPOCH 0 - PROGRESS: at 76.60% examples, 398629 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:02:40,199 : INFO : EPOCH 0 - PROGRESS: at 79.88% examples, 399231 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:41,250 : INFO : EPOCH 0 - PROGRESS: at 83.08% examples, 399119 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:42,258 : INFO : EPOCH 0 - PROGRESS: at 86.06% examples, 398830 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:02:43,268 : INFO : EPOCH 0 - PROGRESS: at 89.11% examples, 398845 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:02:44,268 : INFO : EPOCH 0 - PROGRESS: at 92.15% examples, 399129 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:02:45,290 : INFO : EPOCH 0 - PROGRESS: at 95.22% examples, 399168 words/s, in_qsize 12, out_qsize 0\n",
      "2022-12-21 11:02:46,307 : INFO : EPOCH 0 - PROGRESS: at 98.27% examples, 399437 words/s, in_qsize 11, out_qsize 0\n",
      "2022-12-21 11:02:46,861 : INFO : EPOCH 0: training on 17566362 raw words (13253685 effective words) took 33.2s, 399672 effective words/s\n",
      "2022-12-21 11:02:46,887 : INFO : Doc2Vec lifecycle event {'msg': 'training on 17566362 raw words (13253685 effective words) took 33.2s, 399316 effective words/s', 'datetime': '2022-12-21T11:02:46.887143', 'gensim': '4.2.0', 'python': '3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]', 'platform': 'Linux-6.0.11-1-MANJARO-x86_64-with-glibc2.36', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "model._doc2vec.train(\n",
    "    task.train,\n",
    "    min_doc_id=25000,\n",
    "    max_doc_id=100000 - 1,\n",
    "    epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eaabd8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:02:46,998 : WARNING : Loading cached processed dataset at /home/dburian/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-d3d2c23422642dcc.arrow\n",
      "2022-12-21 11:02:47,281 : WARNING : Parameter 'function'=<function IMDBDoc2Vec._to_tf_dataset.<locals>.<lambda> at 0x7fd4ff7a5900> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29be9b77ecab4e43a2b0b337fb2faa79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2, 800), dtype=float32, numpy=\n",
      "array([[-0.00331919,  0.00081138, -0.00329357, ..., -0.05986395,\n",
      "        -0.15210591, -0.06205365],\n",
      "       [ 0.00268728,  0.00534621,  0.00690543, ..., -0.047584  ,\n",
      "        -0.01430116, -0.04755143]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>)\n",
      "(<tf.Tensor: shape=(2, 800), dtype=float32, numpy=\n",
      "array([[ 0.00857443,  0.01514327,  0.0061665 , ..., -0.05915277,\n",
      "        -0.10532252, -0.04429276],\n",
      "       [-0.0006623 ,  0.0103007 ,  0.01685007, ..., -0.03570338,\n",
      "        -0.05312823, -0.12384202]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)\n",
      "(<tf.Tensor: shape=(2, 800), dtype=float32, numpy=\n",
      "array([[ 3.00449575e-03,  9.32767987e-03,  3.95790965e-04, ...,\n",
      "        -1.37519548e-02, -6.78614378e-02, -5.10153696e-02],\n",
      "       [-2.39831628e-03,  1.62652024e-04,  2.06642468e-02, ...,\n",
      "        -6.72813281e-02, -1.07971355e-01, -1.95203394e-01]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0])>)\n",
      "(<tf.Tensor: shape=(2, 800), dtype=float32, numpy=\n",
      "array([[-0.00118593,  0.00078786, -0.00297773, ..., -0.03249951,\n",
      "        -0.0652047 , -0.04318096],\n",
      "       [ 0.01093145,  0.02220446,  0.00201555, ..., -0.12392675,\n",
      "        -0.06986379, -0.0436304 ]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)\n",
      "(<tf.Tensor: shape=(2, 800), dtype=float32, numpy=\n",
      "array([[-0.00718769, -0.00614955, -0.01390947, ..., -0.08240694,\n",
      "        -0.1445594 , -0.02309898],\n",
      "       [-0.00166996, -0.00607456, -0.01586254, ..., -0.05418707,\n",
      "        -0.09727349, -0.03262834]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:06:56,786 : INFO : {\"jsonrpc\": \"2.0\", \"method\": \"SyncRequest\", \"params\": {\"data\": {\"file_name\": \"/home/dburian/docs/transformer_document_embedding/notebooks/doc2vec_imdb.sync.py\", \"contents\": \"# ---\\n# jupyter:\\n#  jupytext:\\n#   text_representation:\\n#    extension: .py\\n#    format_name: percent\\n#    format_version: '1.3'\\n#    jupytext_version: 1.3.4\\n#  kernelspec:\\n#   display_name: Python 3\\n#   language: python\\n#   name: python3\\n# ---\\n# %%\\nimport logging\\nimport os\\nfrom typing import Any\\n\\nimport transformer_document_embedding as tde\\n\\nos.environ.setdefault(\\\"TF_CPP_MIN_LOG_LEVEL\\\", \\\"2\\\") # Report only TF errors by default\\n\\nimport tensorflow as tf\\n\\nlogging.basicConfig(\\n  format=\\\"%(asctime)s : %(levelname)s : %(message)s\\\", level=logging.INFO\\n)\\n# %%\\nLOG_DIR = \\\"../results/notebooks/doc2vec_imdb\\\"\\ntask = tde.tasks.IMDBClassification()\\nmodel = tde.models.IMDBDoc2Vec(log_dir=LOG_DIR)\\n# %%\\nfor i, doc in enumerate(task.train):\\n  if i < 3:\\n    print(doc)\\n  if 25000 < i < 25003:\\n    print(doc)\\n  if i > 25003:\\n    break\\n\\n# %%\\nmodel._doc2vec.train(\\n  task.train,\\n  min_doc_id=25000,\\n  max_doc_id=100000 - 1,\\n  epochs=1,\\n)\\n# %%\\ntf_ds = model._to_tf_dataset(task.train, training=True).shuffle(25000)\\n\\nfor batch in tf_ds.take(5):\\n  print(batch)\\n\\n# %%\\nmodel._cls_head.fit(\\n  tf_ds, epochs=10, callbacks=[tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)]\\n)\\n# %%\\ntest_predictions = model.predict(task.test)\\nresults = task.evaluate(test_predictions)\\n# %%\\nmodel.dv[25000]\\n# %%\\n\\n\\ndef add_feature_vec(doc: dict[str, Any]) -> dict[str, Any]:\\n  feature_vector = model.dv[doc[\\\"id\\\"]]\\n  return {\\\"features\\\": feature_vector, \\\"label\\\": doc[\\\"label\\\"]}\\n\\n\\nsoftmax_train = train.map(add_feature_vec, remove_columns=[\\\"text\\\", \\\"id\\\"]).to_tf_dataset(\\n  1\\n)\\nsoftmax_train = softmax_train.unbatch()\\nsoftmax_train = softmax_train.map(lambda doc: (doc[\\\"features\\\"], doc[\\\"label\\\"]))\\nsoftmax_train = softmax_train.shuffle(1000)\\nsoftmax_train = softmax_train.batch(2)\\n# tf_dataset_softmax_train = tf.data.Dataset.from_generator(\\n#   lambda: softmax_train,\\n#   output_types=(tf.float32, tf.int32),\\n#   output_shapes=((1,), (0,)),\\n# )\\nfor x in softmax_train.take(5):\\n  print(x)\\n# %%\\nsoftmax_net = tf.keras.Sequential(\\n  [\\n    tf.keras.layers.Input(400),\\n    tf.keras.layers.Dense(50, activation=tf.nn.relu),\\n    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid),\\n  ]\\n)\\nsoftmax_net.compile(\\n  optimizer=tf.keras.optimizers.Adam(),\\n  loss=tf.losses.BinaryCrossentropy(),\\n  metrics=[tf.metrics.BinaryAccuracy()],\\n)\\n# %%\\nsoftmax_net.fit(softmax_train)\\n# %%\\n\"}}, \"id\": 1}\n"
     ]
    }
   ],
   "source": [
    "tf_ds = model._to_tf_dataset(task.train, training=True).shuffle(25000)\n",
    "\n",
    "for batch in tf_ds.take(5):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "714cc800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12500/12500 [==============================] - 16s 797us/step - loss: 0.4194 - binary_accuracy: 0.8016\n",
      "Epoch 2/10\n",
      "12500/12500 [==============================] - 15s 757us/step - loss: 0.2740 - binary_accuracy: 0.8854\n",
      "Epoch 3/10\n",
      "12500/12500 [==============================] - 15s 747us/step - loss: 0.2264 - binary_accuracy: 0.9065\n",
      "Epoch 4/10\n",
      "12500/12500 [==============================] - 15s 751us/step - loss: 0.1955 - binary_accuracy: 0.9224\n",
      "Epoch 5/10\n",
      "12500/12500 [==============================] - 15s 748us/step - loss: 0.1744 - binary_accuracy: 0.9314\n",
      "Epoch 6/10\n",
      "12500/12500 [==============================] - 15s 746us/step - loss: 0.1565 - binary_accuracy: 0.9393\n",
      "Epoch 7/10\n",
      "12500/12500 [==============================] - 15s 771us/step - loss: 0.1456 - binary_accuracy: 0.9438\n",
      "Epoch 8/10\n",
      "12500/12500 [==============================] - 16s 840us/step - loss: 0.1370 - binary_accuracy: 0.9474\n",
      "Epoch 9/10\n",
      "12500/12500 [==============================] - 20s 1ms/step - loss: 0.1259 - binary_accuracy: 0.9511\n",
      "Epoch 10/10\n",
      "12500/12500 [==============================] - 15s 766us/step - loss: 0.1197 - binary_accuracy: 0.9534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3f5422ba30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 10:53:05,273 : INFO : {\"jsonrpc\": \"2.0\", \"method\": \"SyncRequest\", \"params\": {\"data\": {\"file_name\": \"/home/dburian/docs/transformer_document_embedding/notebooks/doc2vec_imdb.sync.py\", \"contents\": \"# ---\\n# jupyter:\\n#  jupytext:\\n#   text_representation:\\n#    extension: .py\\n#    format_name: percent\\n#    format_version: '1.3'\\n#    jupytext_version: 1.3.4\\n#  kernelspec:\\n#   display_name: Python 3\\n#   language: python\\n#   name: python3\\n# ---\\n# %%\\nimport logging\\nimport os\\nfrom typing import Any\\n\\nimport transformer_document_embedding as tde\\n\\nos.environ.setdefault(\\\"TF_CPP_MIN_LOG_LEVEL\\\", \\\"2\\\") # Report only TF errors by default\\n\\nimport tensorflow as tf\\n\\nlogging.basicConfig(\\n  format=\\\"%(asctime)s : %(levelname)s : %(message)s\\\", level=logging.INFO\\n)\\n# %%\\nLOG_DIR = \\\"../results/notebooks/doc2vec_imdb\\\"\\ntask = tde.tasks.IMDBClassification()\\nmodel = tde.models.IMDBDoc2Vec(log_dir=LOG_DIR)\\n# %%\\nfor i, doc in enumerate(task.train):\\n  if i < 3:\\n    print(doc)\\n  if 25000 < i < 25003:\\n    print(doc)\\n  if i > 25003:\\n    break\\n\\n# %%\\nmodel._doc2vec.train(\\n  task.train,\\n  min_doc_id=25000,\\n  max_doc_id=100000 - 1,\\n  epochs=1,\\n)\\n# %%\\ntf_ds = model._to_tf_dataset(task.train, training=True).shuffle(25000)\\n\\nfor batch in tf_ds.take(5):\\n  print(batch)\\n\\n# %%\\nmodel._cls_head.fit(\\n  tf_ds, epochs=10, callbacks=[tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)]\\n)\\n# %%\\ntask.evaluate\\n# %%\\nmodel.dv[25000]\\n# %%\\n\\n\\ndef add_feature_vec(doc: dict[str, Any]) -> dict[str, Any]:\\n  feature_vector = model.dv[doc[\\\"id\\\"]]\\n  return {\\\"features\\\": feature_vector, \\\"label\\\": doc[\\\"label\\\"]}\\n\\n\\nsoftmax_train = train.map(add_feature_vec, remove_columns=[\\\"text\\\", \\\"id\\\"]).to_tf_dataset(\\n  1\\n)\\nsoftmax_train = softmax_train.unbatch()\\nsoftmax_train = softmax_train.map(lambda doc: (doc[\\\"features\\\"], doc[\\\"label\\\"]))\\nsoftmax_train = softmax_train.shuffle(1000)\\nsoftmax_train = softmax_train.batch(2)\\n# tf_dataset_softmax_train = tf.data.Dataset.from_generator(\\n#   lambda: softmax_train,\\n#   output_types=(tf.float32, tf.int32),\\n#   output_shapes=((1,), (0,)),\\n# )\\nfor x in softmax_train.take(5):\\n  print(x)\\n# %%\\nsoftmax_net = tf.keras.Sequential(\\n  [\\n    tf.keras.layers.Input(400),\\n    tf.keras.layers.Dense(50, activation=tf.nn.relu),\\n    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid),\\n  ]\\n)\\nsoftmax_net.compile(\\n  optimizer=tf.keras.optimizers.Adam(),\\n  loss=tf.losses.BinaryCrossentropy(),\\n  metrics=[tf.metrics.BinaryAccuracy()],\\n)\\n# %%\\nsoftmax_net.fit(softmax_train)\\n# %%\\n\"}}, \"id\": 1}\n"
     ]
    }
   ],
   "source": [
    "model._cls_head.fit(\n",
    "    tf_ds, epochs=10, callbacks=[tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca895bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:06:59,616 : WARNING : Found cached dataset imdb (/home/dburian/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "2022-12-21 11:06:59,669 : WARNING : Loading cached processed dataset at /home/dburian/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-072999855889cdf5.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a5af1eeda7446faf61303ae3fa26c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500/12500 [==============================] - 12s 919us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:07:31,834 : INFO : {\"jsonrpc\": \"2.0\", \"method\": \"SyncRequest\", \"params\": {\"data\": {\"file_name\": \"/home/dburian/docs/transformer_document_embedding/notebooks/doc2vec_imdb.sync.py\", \"contents\": \"# ---\\n# jupyter:\\n#  jupytext:\\n#   text_representation:\\n#    extension: .py\\n#    format_name: percent\\n#    format_version: '1.3'\\n#    jupytext_version: 1.3.4\\n#  kernelspec:\\n#   display_name: Python 3\\n#   language: python\\n#   name: python3\\n# ---\\n# %%\\nimport logging\\nimport os\\nfrom typing import Any\\n\\nimport transformer_document_embedding as tde\\n\\nos.environ.setdefault(\\\"TF_CPP_MIN_LOG_LEVEL\\\", \\\"2\\\") # Report only TF errors by default\\n\\nimport tensorflow as tf\\n\\nlogging.basicConfig(\\n  format=\\\"%(asctime)s : %(levelname)s : %(message)s\\\", level=logging.INFO\\n)\\n# %%\\nLOG_DIR = \\\"../results/notebooks/doc2vec_imdb\\\"\\ntask = tde.tasks.IMDBClassification()\\nmodel = tde.models.IMDBDoc2Vec(log_dir=LOG_DIR)\\n# %%\\nfor i, doc in enumerate(task.train):\\n  if i < 3:\\n    print(doc)\\n  if 25000 < i < 25003:\\n    print(doc)\\n  if i > 25003:\\n    break\\n\\n# %%\\nmodel._doc2vec.train(\\n  task.train,\\n  min_doc_id=25000,\\n  max_doc_id=100000 - 1,\\n  epochs=1,\\n)\\n# %%\\ntf_ds = model._to_tf_dataset(task.train, training=True).shuffle(25000)\\n\\nfor batch in tf_ds.take(5):\\n  print(batch)\\n\\n# %%\\nmodel._cls_head.fit(\\n  tf_ds, epochs=10, callbacks=[tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)]\\n)\\n# %%\\ntest_predictions = model.predict(task.test)\\nresults = task.evaluate(test_predictions)\\n# %%\\nprint(results)\\n# %%\\nmodel.dv[25000]\\n# %%\\n\\n\\ndef add_feature_vec(doc: dict[str, Any]) -> dict[str, Any]:\\n  feature_vector = model.dv[doc[\\\"id\\\"]]\\n  return {\\\"features\\\": feature_vector, \\\"label\\\": doc[\\\"label\\\"]}\\n\\n\\nsoftmax_train = train.map(add_feature_vec, remove_columns=[\\\"text\\\", \\\"id\\\"]).to_tf_dataset(\\n  1\\n)\\nsoftmax_train = softmax_train.unbatch()\\nsoftmax_train = softmax_train.map(lambda doc: (doc[\\\"features\\\"], doc[\\\"label\\\"]))\\nsoftmax_train = softmax_train.shuffle(1000)\\nsoftmax_train = softmax_train.batch(2)\\n# tf_dataset_softmax_train = tf.data.Dataset.from_generator(\\n#   lambda: softmax_train,\\n#   output_types=(tf.float32, tf.int32),\\n#   output_shapes=((1,), (0,)),\\n# )\\nfor x in softmax_train.take(5):\\n  print(x)\\n# %%\\nsoftmax_net = tf.keras.Sequential(\\n  [\\n    tf.keras.layers.Input(400),\\n    tf.keras.layers.Dense(50, activation=tf.nn.relu),\\n    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid),\\n  ]\\n)\\nsoftmax_net.compile(\\n  optimizer=tf.keras.optimizers.Adam(),\\n  loss=tf.losses.BinaryCrossentropy(),\\n  metrics=[tf.metrics.BinaryAccuracy()],\\n)\\n# %%\\nsoftmax_net.fit(softmax_train)\\n# %%\\n\"}}, \"id\": 1}\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict(task.test)\n",
    "results = task.evaluate(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec328489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'binary_crossentropy': 0.6931428, 'binary_accuracy': 0.50212}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:38:40,819 : INFO : {\"jsonrpc\": \"2.0\", \"method\": \"SyncRequest\", \"params\": {\"data\": {\"file_name\": \"/home/dburian/docs/transformer_document_embedding/notebooks/doc2vec_imdb.sync.py\", \"contents\": \"# ---\\n# jupyter:\\n#  jupytext:\\n#   text_representation:\\n#    extension: .py\\n#    format_name: percent\\n#    format_version: '1.3'\\n#    jupytext_version: 1.3.4\\n#  kernelspec:\\n#   display_name: Python 3\\n#   language: python\\n#   name: python3\\n# ---\\n# %%\\nimport logging\\nimport os\\nfrom typing import Any\\n\\nimport transformer_document_embedding as tde\\n\\nos.environ.setdefault(\\\"TF_CPP_MIN_LOG_LEVEL\\\", \\\"2\\\") # Report only TF errors by default\\n\\nimport tensorflow as tf\\n\\nlogging.basicConfig(\\n  format=\\\"%(asctime)s : %(levelname)s : %(message)s\\\", level=logging.INFO\\n)\\n# %%\\nimport importlib\\n\\nimportlib.reload(tde)\\n\\n# %%\\nLOG_DIR = \\\"../results/notebooks/doc2vec_imdb\\\"\\ntask = tde.tasks.IMDBClassification()\\nmodel = tde.models.IMDBDoc2Vec(log_dir=LOG_DIR)\\n# %%\\nfor i, doc in enumerate(task.train):\\n  if i < 3:\\n    print(doc)\\n  if 25000 < i < 25003:\\n    print(doc)\\n  if i > 25003:\\n    break\\n\\n# %%\\nmodel._doc2vec.train(\\n  task.train,\\n  min_doc_id=25000,\\n  max_doc_id=100000 - 1,\\n  epochs=1,\\n)\\n# %%\\ntf_ds = model._to_tf_dataset(task.train, training=True).shuffle(25000)\\n\\nfor batch in tf_ds.take(5):\\n  print(batch)\\n\\n# %%\\nmodel._cls_head.fit(\\n  tf_ds, epochs=10, callbacks=[tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)]\\n)\\n# %%\\ntest_predictions = model.predict(task.test)\\nresults = task.evaluate(test_predictions)\\n# %%\\nprint(results)\\ntde.evaluation.save_results(results, LOG_DIR)\\n# %%\\ntrain.task[\\\"label\\\"]\\n\\n# %%\\n# %%\\n\"}}, \"id\": 1}\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "tde.evaluation.save_results(results, LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d121de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "task.train\n",
    "a = task.train[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50e287da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a bad movie. Not one of the funny bad ones either. This is a lousy bad one. It was actually painful to watch. The direction was awful,with lots of jumping around and the green and yellow hues used throughout the movie makes the characters look sickly. Keira Knightly was not convincing as a tough chick at all,and I cannot believe Lucy Liu and Mickey Rourke signed on for this criminal waste of celluloid. The script was terrible and the acting was like fingernails across a chalkboard. If you haven't seen it,don't. You are not missing anything and will only waste two hours of your life watching this drivel .I have seen bad movies before and even enjoyed them due to their faults. This one is just a waste of time.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 12:08:10,555 : INFO : {\"jsonrpc\": \"2.0\", \"method\": \"SyncRequest\", \"params\": {\"data\": {\"file_name\": \"/home/dburian/docs/transformer_document_embedding/notebooks/doc2vec_imdb.sync.py\", \"contents\": \"# ---\\n# jupyter:\\n#  jupytext:\\n#   text_representation:\\n#    extension: .py\\n#    format_name: percent\\n#    format_version: '1.3'\\n#    jupytext_version: 1.3.4\\n#  kernelspec:\\n#   display_name: Python 3\\n#   language: python\\n#   name: python3\\n# ---\\n# %%\\nimport logging\\nimport os\\nfrom typing import Any\\n\\nimport transformer_document_embedding as tde\\n\\nos.environ.setdefault(\\\"TF_CPP_MIN_LOG_LEVEL\\\", \\\"2\\\") # Report only TF errors by default\\n\\nimport tensorflow as tf\\n\\nlogging.basicConfig(\\n  format=\\\"%(asctime)s : %(levelname)s : %(message)s\\\", level=logging.INFO\\n)\\n# %%\\nimport importlib\\n\\nimportlib.reload(tde)\\n\\n# %%\\nLOG_DIR = \\\"../results/notebooks/doc2vec_imdb\\\"\\ntask = tde.tasks.IMDBClassification()\\nmodel = tde.models.IMDBDoc2Vec(log_dir=LOG_DIR)\\n# %%\\nfor i, doc in enumerate(task.train):\\n  if i < 3:\\n    print(doc)\\n  if 25000 < i < 25003:\\n    print(doc)\\n  if i > 25003:\\n    break\\n\\n# %%\\nmodel._doc2vec.train(\\n  task.train,\\n  min_doc_id=25000,\\n  max_doc_id=100000 - 1,\\n  epochs=1,\\n)\\n# %%\\ntf_ds = model._to_tf_dataset(task.train, training=True).shuffle(25000)\\n\\nfor batch in tf_ds.take(5):\\n  print(batch)\\n\\n# %%\\nmodel._cls_head.fit(\\n  tf_ds, epochs=10, callbacks=[tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)]\\n)\\n# %%\\ntest_predictions = model.predict(task.test)\\nresults = task.evaluate(test_predictions)\\n# %%\\nprint(results)\\ntde.evaluation.save_results(results, LOG_DIR)\\n# %%\\ntask.train\\na = task.train[\\\"label\\\"]\\n# %%\\na[3000]\\n# %%\\ndata = task.train.filter(lambda doc: doc[\\\"label\\\"] >= 0)\\nfeatures_iter = model._doc2vec.predict(data)\\n# %%\\n\"}}, \"id\": 1}\n"
     ]
    }
   ],
   "source": [
    "a[3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26602a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795a54f6602049b3916b50006f5947e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 12:10:53,902 : INFO : {\"jsonrpc\": \"2.0\", \"method\": \"SyncRequest\", \"params\": {\"data\": {\"file_name\": \"/home/dburian/docs/transformer_document_embedding/notebooks/doc2vec_imdb.sync.py\", \"contents\": \"# ---\\n# jupyter:\\n#  jupytext:\\n#   text_representation:\\n#    extension: .py\\n#    format_name: percent\\n#    format_version: '1.3'\\n#    jupytext_version: 1.3.4\\n#  kernelspec:\\n#   display_name: Python 3\\n#   language: python\\n#   name: python3\\n# ---\\n# %%\\nimport logging\\nimport os\\nfrom typing import Any\\n\\nimport datasets\\n\\nimport transformer_document_embedding as tde\\n\\nos.environ.setdefault(\\\"TF_CPP_MIN_LOG_LEVEL\\\", \\\"2\\\") # Report only TF errors by default\\n\\nimport tensorflow as tf\\n\\nlogging.basicConfig(\\n  format=\\\"%(asctime)s : %(levelname)s : %(message)s\\\", level=logging.INFO\\n)\\n# %%\\nimport importlib\\n\\nimportlib.reload(tde)\\n\\n# %%\\nLOG_DIR = \\\"../results/notebooks/doc2vec_imdb\\\"\\ntask = tde.tasks.IMDBClassification()\\nmodel = tde.models.IMDBDoc2Vec(log_dir=LOG_DIR)\\n# %%\\nfor i, doc in enumerate(task.train):\\n  if i < 3:\\n    print(doc)\\n  if 25000 < i < 25003:\\n    print(doc)\\n  if i > 25003:\\n    break\\n\\n# %%\\nmodel._doc2vec.train(\\n  task.train,\\n  min_doc_id=25000,\\n  max_doc_id=100000 - 1,\\n  epochs=1,\\n)\\n# %%\\ntf_ds = model._to_tf_dataset(task.train, training=True).shuffle(25000)\\n\\nfor batch in tf_ds.take(5):\\n  print(batch)\\n\\n# %%\\nmodel._cls_head.fit(\\n  tf_ds, epochs=10, callbacks=[tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)]\\n)\\n# %%\\ntest_predictions = model.predict(task.test)\\nresults = task.evaluate(test_predictions)\\n# %%\\nprint(results)\\ntde.evaluation.save_results(results, LOG_DIR)\\n# %%\\ntask.train\\na = task.train[\\\"label\\\"]\\n# %%\\na[3000]\\n# %%\\ndata = task.train.filter(lambda doc: doc[\\\"label\\\"] >= 0)\\nfeatures_iter = model._doc2vec.predict(data)\\n# %%\\nfeatures_dataset = datasets.Dataset.from_generator(features_iter)\\n\"}}, \"id\": 1}\n"
     ]
    }
   ],
   "source": [
    "data = task.train.filter(lambda doc: doc[\"label\"] >= 0)\n",
    "features_iter = model._doc2vec.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eac216c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557869997c1d4415929429ea8c848680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 12:17:36,021 : INFO : {\"jsonrpc\": \"2.0\", \"method\": \"SyncRequest\", \"params\": {\"data\": {\"file_name\": \"/home/dburian/docs/transformer_document_embedding/notebooks/doc2vec_imdb.sync.py\", \"contents\": \"# ---\\n# jupyter:\\n#  jupytext:\\n#   text_representation:\\n#    extension: .py\\n#    format_name: percent\\n#    format_version: '1.3'\\n#    jupytext_version: 1.3.4\\n#  kernelspec:\\n#   display_name: Python 3\\n#   language: python\\n#   name: python3\\n# ---\\n# %%\\nimport logging\\nimport os\\nfrom typing import Any\\n\\nimport datasets\\n\\nimport transformer_document_embedding as tde\\n\\nos.environ.setdefault(\\\"TF_CPP_MIN_LOG_LEVEL\\\", \\\"2\\\") # Report only TF errors by default\\n\\nimport tensorflow as tf\\n\\nlogging.basicConfig(\\n  format=\\\"%(asctime)s : %(levelname)s : %(message)s\\\", level=logging.INFO\\n)\\n# %%\\nimport importlib\\n\\nimportlib.reload(tde)\\n\\n# %%\\nLOG_DIR = \\\"../results/notebooks/doc2vec_imdb\\\"\\ntask = tde.tasks.IMDBClassification()\\nmodel = tde.models.IMDBDoc2Vec(log_dir=LOG_DIR)\\n# %%\\nfor i, doc in enumerate(task.train):\\n  if i < 3:\\n    print(doc)\\n  if 25000 < i < 25003:\\n    print(doc)\\n  if i > 25003:\\n    break\\n\\n# %%\\nmodel._doc2vec.train(\\n  task.train,\\n  min_doc_id=25000,\\n  max_doc_id=100000 - 1,\\n  epochs=1,\\n)\\n# %%\\ntf_ds = model._to_tf_dataset(task.train, training=True).shuffle(25000)\\n\\nfor batch in tf_ds.take(5):\\n  print(batch)\\n\\n# %%\\nmodel._cls_head.fit(\\n  tf_ds, epochs=10, callbacks=[tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)]\\n)\\n# %%\\ntest_predictions = model.predict(task.test)\\nresults = task.evaluate(test_predictions)\\n# %%\\nprint(results)\\ntde.evaluation.save_results(results, LOG_DIR)\\n# %%\\ntask.train\\na = task.train[\\\"label\\\"]\\n# %%\\na[3000]\\n# %%\\ndata = task.train.filter(lambda doc: doc[\\\"label\\\"] >= 0)\\nfeatures_iter = model._doc2vec.predict(data)\\n# %%\\ndata = data.map(\\n  lambda doc: {\\\"features\\\": next(features_iter)},\\n  keep_in_memory=True,\\n  load_from_cache_file=False,\\n)\\n# %%\\ntry:\\n  print(next(features_iter))\\nexcept StopIteration:\\n  print(\\\"no more features\\\")\\n# %%\\nfeatures_dataset = datasets.Dataset.from_generator(features_iter)\\n\"}}, \"id\": 1}\n"
     ]
    }
   ],
   "source": [
    "data = data.map(\n",
    "    lambda _: {\"features\": next(features_iter)},\n",
    "    keep_in_memory=True,\n",
    "    load_from_cache_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ae4bb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no more features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 12:18:57,031 : INFO : {\"jsonrpc\": \"2.0\", \"method\": \"SyncRequest\", \"params\": {\"data\": {\"file_name\": \"/home/dburian/docs/transformer_document_embedding/notebooks/doc2vec_imdb.sync.py\", \"contents\": \"# ---\\n# jupyter:\\n#  jupytext:\\n#   text_representation:\\n#    extension: .py\\n#    format_name: percent\\n#    format_version: '1.3'\\n#    jupytext_version: 1.3.4\\n#  kernelspec:\\n#   display_name: Python 3\\n#   language: python\\n#   name: python3\\n# ---\\n# %%\\nimport logging\\nimport os\\nfrom typing import Any\\n\\nimport datasets\\n\\nimport transformer_document_embedding as tde\\n\\nos.environ.setdefault(\\\"TF_CPP_MIN_LOG_LEVEL\\\", \\\"2\\\") # Report only TF errors by default\\n\\nimport tensorflow as tf\\n\\nlogging.basicConfig(\\n  format=\\\"%(asctime)s : %(levelname)s : %(message)s\\\", level=logging.INFO\\n)\\n# %%\\nimport importlib\\n\\nimportlib.reload(tde)\\n\\n# %%\\nLOG_DIR = \\\"../results/notebooks/doc2vec_imdb\\\"\\ntask = tde.tasks.IMDBClassification()\\nmodel = tde.models.IMDBDoc2Vec(log_dir=LOG_DIR)\\n# %%\\nfor i, doc in enumerate(task.train):\\n  if i < 3:\\n    print(doc)\\n  if 25000 < i < 25003:\\n    print(doc)\\n  if i > 25003:\\n    break\\n\\n# %%\\nmodel._doc2vec.train(\\n  task.train,\\n  min_doc_id=25000,\\n  max_doc_id=100000 - 1,\\n  epochs=1,\\n)\\n# %%\\ntf_ds = model._to_tf_dataset(task.train, training=True).shuffle(25000)\\n\\nfor batch in tf_ds.take(5):\\n  print(batch)\\n\\n# %%\\nmodel._cls_head.fit(\\n  tf_ds, epochs=10, callbacks=[tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)]\\n)\\n# %%\\ntest_predictions = model.predict(task.test)\\nresults = task.evaluate(test_predictions)\\n# %%\\nprint(results)\\ntde.evaluation.save_results(results, LOG_DIR)\\n# %%\\ntask.train\\na = task.train[\\\"label\\\"]\\n# %%\\na[3000]\\n# %%\\ndata = task.train.filter(lambda doc: doc[\\\"label\\\"] >= 0)\\nfeatures_iter = model._doc2vec.predict(data)\\n# %%\\ndata = data.map(\\n  lambda doc: {\\\"features\\\": next(features_iter)},\\n  keep_in_memory=True,\\n  load_from_cache_file=False,\\n)\\n# %%\\ntry:\\n  print(next(features_iter))\\nexcept StopIteration:\\n  print(\\\"no more features\\\")\\n\"}}, \"id\": 1}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(next(features_iter))\n",
    "except StopIteration:\n",
    "    print(\"no more features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
