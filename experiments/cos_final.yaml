model:
  module: transformer:TransformerEmbedder
  kwargs:
    transformer_name: allenai/longformer-base-4096
    pooler_type: mean

head:
  module: structural_contextual_head:StructuralContextualHead
  kwargs:
    max_structural_length: 384
    lam: 0.5
    structural_head_kwargs:
      loss_type: cos_dist
      max_marginals_lam: 1
    contextual_head_kwargs:
      contextual_dim: 2048
      student_projection:
        - features: 768
          activation: relu
        - features: 4096
          activation: relu
        - features: 2048
      contextual_projection: []
      loss_type: soft_cca
      soft_cca_lam: 0.03
      soft_cca_sdl_alpha: 0.95

train_pipeline:
  kind: student
  kwargs:
    batch_size: 6
    epochs: 1
    warmup_steps: 5000
    grad_accumulation_steps: 10
    weight_decay: 0.01
    fp16: True
    lr_scheduler_type: cos
    lr: 3.0e-5
    max_grad_norm: 1.0
    log_every_step: 100
    validate_every_step: 25000
    save_best: False
    patience: null
    global_attention_type: none
    dataloader_sampling: consistent
    metric_window_size_mult: 5
    metric_window_shift_frac: 1.0
    save_after_steps:
      - 41666
      - 83333
      - 125000
    sampler_kwargs:
      bucket_limits: [512, 1024, 1536, 2048, 2560, 3072, 3584, 4096]

dataset:
  module: teacher_embedding:TeacherEmbedding
  kwargs:
    path: ./data/train_1500k
    contextual_embedding_col: pv_2048
    structural_embedding_col: sbert
    data_size_limit:
      train: 1000000
