model:
  module: transformer:TransformerEmbedder
  kwargs:
    transformer_name: allenai/longformer-base-4096
    pooler_type: mean

head:
  module: structural_contextual_head:StructuralContextualHead
  kwargs:
    max_structural_length: null
    lam: 1.0
    structural_head_kwargs:
      loss_type: max_marginals_mse
      max_marginals_lam: 1
    contextual_head_kwargs: null

train_pipeline:
  kind: student
  kwargs:
    batch_size: 6
    epochs: 1
    warmup_steps: 5000
    grad_accumulation_steps: 10
    weight_decay: 0.01
    fp16: True
    lr_scheduler_type: cos
    lr: 3.0e-5
    max_grad_norm: 1.0
    log_every_step: 100
    validate_every_step: 25000
    save_best: False
    patience: null
    global_attention_type: none
    dataloader_sampling: consistent
    metric_window_size_mult: 5
    metric_window_shift_frac: 1.0
    save_after_steps:
      - 41666
      - 83333
      - 125000
    sampler_kwargs:
      bucket_limits: [512, 1024, 1536, 2048, 2560, 3072, 3584, 4096]

dataset:
  module: teacher_embedding:TeacherEmbedding
  kwargs:
    path: ./data/train_1500k
    contextual_embedding_col: dm_100d
    structural_embedding_col: sbert
    data_size_limit:
      train: 1000000
