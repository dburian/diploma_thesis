model:
  module: transformer:TransformerEmbedder
  kwargs:
    transformer_name: allenai/longformer-base-4096
    # transformer_name: sentence-transformers/all-mpnet-base-v2
    pooler_type: mean

head:
  module: structural_contextual_head:StructuralContextualHead
  kwargs:
    max_structural_length: null
    lam: 1
    structural_head_kwargs:
      loss_type: max_marginals_cos_dist
      max_marginals_lam: 1
    contextual_head_kwargs:
      contextual_dim: 100
      student_projection:
        - features: 256
          activation: relu
        - features: 768
      contextual_projection:
        - features: 768
      loss_type: soft_cca
      soft_cca_lam: 0.03
      soft_cca_sdl_alpha: 0.95


train_pipeline:
  kind: student
  kwargs:
    batch_size: 3
    epochs: 1
    warmup_steps: 100
    grad_accumulation_steps: 1
    weight_decay: 0.01
    fp16: True
    lr_scheduler_type: cos
    lr: 1.0e-4
    max_grad_norm: 1.0
    log_every_step: 1
    validate_every_step: 100
    save_best: False
    patience: null
    global_attention_type: cls
    dataloader_sampling: default
    metric_window_size_mult: 5
    metric_window_shift_frac: 0.1
    sampler_kwargs: null
      #bucket_limits: [512, 1024, 1536, 2048, 2560, 3072, 3584, 4096]

dataset:
  module: teacher_embedding:TeacherEmbedding
  kwargs:
    path: /mnt/data/datasets/wikipedia_resampled_eval
    contextual_embedding_col: dbow
    structural_embedding_col: hf_sbert
    data_size_limit:
      train: 1000
      validation: 7680
      test: 50
