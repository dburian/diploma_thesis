model:
  module: transformer.student:TransformerStudent
  kwargs:
    # transformer_model: google/bigbird-roberta-base
    transformer_model: sentence-transformers/all-mpnet-base-v2
    # transformer_model: allenai/longformer-base-4096
    pooler_type: mean
    batch_size: 4
    # max_depth_length: null
    max_depth_length: 385
    # depth_loss_kwargs: null
    depth_loss_kwargs:
      lam: 1000
      loss_type: mse
      contrastive_lam: 0.8
    breadth_loss_kwargs:
      breadth_embed_dim: 100
      loss_type: soft_cca
      cca_output_dim: null
      # cca_output_dim: 50
      transformer_projection:
        - features: 768
          activation: null
          normalization: null
      breadth_projection:
        - features: 512
          activation: null
          normalization: null
        - features: 768
          activation: null
          normalization: null
      soft_cca_lam: 0.15
      soft_cca_sdl_alpha: 0.999
      contrastive_lam: 0.8

  train_kwargs:
    save_best: False
    epochs: 1
    warmup_steps: 840
    grad_accumulation_steps: 4
    weight_decay: 0.01
    lr: 3.0e-5
    lr_scheduler_type: cos
    fp16: True
    max_grad_norm: 1.0
    log_every_step: 4
    validate_every_step: 1050
    global_attention_type: cls
    patience: null
    device: cuda
    save_after_steps: null
    dataloader_sampling: consistent
    bucket_limits: [1024, 2048, 3072, 4096]



task:
  module: teacher_embedding:TeacherEmbedding
  kwargs:
    path: /mnt/data/datasets/wikipedia_sample_with_eval
    # path: ./results/teacher_embedding:TeacherEmbedding/transformer.student:TransformerStudent/sbert_test/embeddings
    depth_embedding_col: sbert
    breadth_embedding_col: dbow
    data_size_limit:
      train: 200
      validation: 100
